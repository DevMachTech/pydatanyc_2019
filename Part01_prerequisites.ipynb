{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Foreword\n",
    "\n",
    "<center><a href=\"https://twitter.com/emilymbender/status/1067108757488848896?ref_src=twsrc%5Etfw\"><img src=\"img/bender.png\" /></a></blockquote></center>\n",
    "\n",
    "\n",
    "### 1. All datasets are in German\n",
    "### 2. All links are to open access journals _only_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prerequisites\n",
    "\n",
    "Neural networks have four building blocks that define how the network operates and how it is trained. As the network architectures have become increasingly complex the details of how the networks are trained have similarly increased, how large the mini batches are, what optimizer to use etc. For instance, see [_RoBERTa: A Robustly Optimized BERT Pretraining Approach_](https://arxiv.org/abs/1907.11692).\n",
    "\n",
    "The four main components of a trained neural network are\n",
    "\n",
    "- the network architecture\n",
    "    - feed forward, recurrent, transformer?\n",
    "    - how many layers?\n",
    "    - how wide are the layers?\n",
    "- the optimizer\n",
    "    - SGD\n",
    "    - Adam\n",
    "    - AdamW\n",
    "- the scheduler\n",
    "    - constant\n",
    "    - slanted learning rates\n",
    "- the learning objective and loss function\n",
    "    - this is problem dependent\n",
    "    - masked language model (MLM)\n",
    "    - causal language model (CLM)\n",
    "    - translation language model (TLM)\n",
    "    - classification\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html\n",
    "\n",
    "\n",
    "## 1.1 Network Architecture\n",
    "\n",
    "The networks architecture is perhaps the most visible part of the four. It defines how the individual (artificial) neurons work and how they connect together. At its simplest the network is a fully connected feed forward network with sigmoid as the activation function for the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49201864, 0.50798136],\n",
       "       [0.4932044 , 0.50679564],\n",
       "       [0.49051538, 0.5094846 ],\n",
       "       [0.49539065, 0.50460935],\n",
       "       [0.4970286 , 0.5029715 ],\n",
       "       [0.5063873 , 0.4936127 ],\n",
       "       [0.4970608 , 0.5029392 ],\n",
       "       [0.4944676 , 0.50553244],\n",
       "       [0.50102276, 0.49897727],\n",
       "       [0.49211037, 0.50788957]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, layers, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # create feed forward layers\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_features=in_features, out_features=out_features)\n",
    "                                     for (in_features, out_features) in layers])\n",
    "        \n",
    "        # TODO: initialise weights\n",
    "        \n",
    "        # create activation functions for each layer\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, labels=None):\n",
    "        output = None\n",
    "        for i_layer, layer in enumerate(self.layers):\n",
    "            if output is None:\n",
    "                output = self.activation(layer(X))\n",
    "            else:\n",
    "                output = self.activation(layer(output))\n",
    "        # do softmax\n",
    "        prob = F.log_softmax(output, dim=1).exp()\n",
    "        return prob\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "n_features = 50\n",
    "layer_spec = [(n_features, 25), (25, n_classes)]\n",
    "ff = FeedForward(layers=layer_spec)\n",
    "\n",
    "inputs = torch.randn((10, n_features), dtype=torch.float32)\n",
    "ff(inputs).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimizer and Scheduler\n",
    "\n",
    "- http://ruder.io/optimizing-gradient-descent/\n",
    "- http://cs231n.github.io/optimization-1/\n",
    "\n",
    "## 1.3 Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "X, y = make_blobs(n_samples=500, n_features=n_features, centers=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "sampler = RandomSampler(train_dataset)\n",
    "\n",
    "# Utilize the data loader from pytorch to create batches of data \n",
    "train_data = DataLoader(train_dataset, sampler=sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 A Learning Function\n",
    "Define a training function that loops through the training data some number of times and updates the model weights as well as the learning rate schedule.\n",
    "\n",
    "__Note__ that each iteration over the inner for loop takes _one batch_ of data from the `pytorch` data loader class. The batch size determines how much data is used to compute the gradient. ___This will become important later!___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                data: DataLoader,\n",
    "                criterion: nn.modules.loss._Loss,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "                n_epochs: int = 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    # for each epoch\n",
    "    for i_epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # go through the training data one batch at a time\n",
    "        for x_, y_ in data:\n",
    "            # Forward pass\n",
    "            y_pred = model(x_)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), y_)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backprop loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update learning rate schedule\n",
    "            scheduler.step()\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model = FeedForward(layer_spec)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "scheduler = LambdaLR(optimizer, lambda epoch: 1e-3)\n",
    "\n",
    "trained_model, losses_sgd = train_model(model, train_data, criterion, optimizer, scheduler, n_epochs=15)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "trained_model, losses_adam = train_model(model, train_data, criterion, optimizer, scheduler, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of optimizer makes quite a difference. As the network becomes larger (more parameters) and the problme more complex the optimizer becomes increasingly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losses_sgd, label='SGD');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_adam, label='Adam');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1.4 Natural Language (text) to Linear Algebra\n",
    "\n",
    "The dummy data set we used in the previous section was already vectors so that no work was required to transform the data into something (a vector) a neural network can consume. Natural langauge text, or `python` source code for that matter, does not come in this readily available form so some amount of preprocessing is required.\n",
    "\n",
    "The great promise of artificial neural networks for NLP is that a lot of the feature engineering work that has in the past been done by arduous PhD students can be offloaded to the combination of neural network machinery, gradient descent and the loss function to be minimised. Optimising a loss function given some training data may lead to problems down the line in the networks ability to [generalise to other kinds of data](https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/), but we'll forego that concern here.\n",
    "\n",
    "## 1.4.1 Representing Natural Language - a Computer Science Perspective\n",
    "\n",
    "Natural languages are ambiguous, complex and highly expressive ways of encoding information, not just semantics but also a lot of cultural details like class hierarchies and history. The Viking conquests are present in the everyday usage of english. We grow _cows_ _pigs_ and _lamb_ but eat _beef_ _pork_ and _mutton_, at least the non-vegetarians do. In finnish _vasta_ and _vihta_ refer to a collection of young leafy birch branches used when bathing (sauna), however _vihta_ is only ever used in western Finland and _vasta_ only in eastern Finland. Whether or not these etymological tidbits and subtle differences in the semantics of the words are necessary or even helpful in solving some task remains debatable.\n",
    "\n",
    "Word vectors, also known as _embeddings_, aim to create word representations that capture distributional similarity between words as vectors. Specifically, the word vectors should act in such a way that algebraic modifications maintain the semantic coherence. By capturing distributional similarity they end up capturing semantic information about words and their relations and thus allow for a somewhat coherent way of encoding natural language text into vectors.\n",
    "\n",
    "These vectors can be used to encode documents for neural networks.\n",
    "\n",
    "## 1.4.2 A Text Classifier with Embeddings\n",
    "\n",
    "Below, we'll use the fastText word embeddings to create a document classifier for the \"_10k German News Articles Dataset_\" (GNAD) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1218M  100 1218M    0     0  7814k      0  0:02:39  0:02:39 --:--:-- 7518k 0:02:25  0:01:52  0:00:33 8636k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   131  100   131    0     0    627      0 --:--:-- --:--:-- --:--:--   629\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://github.com/tblock/10kGNAD/raw/master/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   130  100   130    0     0    167      0 --:--:-- --:--:-- --:--:--   167\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://github.com/tblock/10kGNAD/raw/master/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de_core_news_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.0/de_core_news_sm-2.2.0.tar.gz#egg=de_core_news_sm==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.0/de_core_news_sm-2.2.0.tar.gz (14.9MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9MB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from de_core_news_sm==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (7.1.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (0.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (1.17.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from spacy>=2.2.0->de_core_news_sm==2.2.0) (0.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->de_core_news_sm==2.2.0) (4.36.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_sm==2.2.0) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_sm==2.2.0) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_sm==2.2.0) (3.0.4)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.0-cp37-none-any.whl size=14912286 sha256=5de6ce5f915611662cf498cdfe4f0ccd43ff647bc3068abb6902928b045e5686\n",
      "  Stored in directory: /private/var/folders/v0/zh1ybpwd1xgb9mrkp8mw1zsr0000gn/T/pip-ephem-wheel-cache-fztoq3uc/wheels/71/3d/6c/0e841e5195cedb751569abff1815e5d890f2c44e72849d3da8\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "ft_vec = KeyedVectors.load_word2vec_format('cc.de.300.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much has been said about word vectors, and as a result I won't go into a lot of detail here, but instead simply refer to the existing excellent resources that are already available.\n",
    "\n",
    "- Noah Smith: [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/abs/1902.06006)\n",
    "\n",
    "\n",
    "- Mikolov et al.: [_Efficient Estimation of Word Representations in Vector Space._](http://arxiv.org/abs/1301.3781) ICLR (Workshop Poster) 2013\n",
    "\n",
    "  > \"_We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities._\"\n",
    "\n",
    "- Mikolov et al. [_Distributed Representations of Words and Phrases and their Compositionality._](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) NIPS 2013: 3111-3119\n",
    "\n",
    "- Le et al. [_Distributed Representations of Sentences and Documents._](http://proceedings.mlr.press/v32/le14.html) ICML 2014: 1188-1196\n",
    "\n",
    "\n",
    "- Chris Olah [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) (July 7, 2014)\n",
    "\n",
    "- Arora et al. [_A Latent Variable Model Approach to PMI-based Word Embeddings._](https://transacl.org/ojs/index.php/tacl/article/view/742) TACL 4: 385-399 (2016)\n",
    "  > Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods.\n",
    "\n",
    "  > This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers.\n",
    "\n",
    "  - Sanjeev Arora [Semantic Word Embeddings](http://www.offconvex.org/2015/12/12/word-embeddings-1/) (Dec 12, 2015)\n",
    "  - Sanjeev Arora [Word Embeddings: Explaining their properties](https://www.offconvex.org/2016/02/14/word-embeddings-2/) (February 14, 2016)\n",
    "\n",
    "---\n",
    "\n",
    "![word2vec](./img/word2vec.png)\n",
    "\n",
    "<sub>From: Mikolov et al.: [_Efficient Estimation of Word Representations in Vector Space._](http://arxiv.org/abs/1301.3781) ICLR (Workshop Poster) 2013</sub>\n",
    "\n",
    "In short word vectors are a way of capturing and representing information about distributional semantics in a way that is convenient for computers to deal with. More to the point, they allow arithmetic operations that are, at least to an extent, semantically coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Potsdam', 0.7689189314842224),\n",
       " ('Berlin-Mitte', 0.7297649383544922),\n",
       " ('Charlottenburg', 0.7252569794654846),\n",
       " ('Neukölln', 0.7029827833175659),\n",
       " ('Hamburg', 0.7024208903312683),\n",
       " ('Brandenburg', 0.7002496719360352),\n",
       " ('Wilmersdorf', 0.6893421411514282),\n",
       " ('Spandau', 0.683754563331604),\n",
       " ('BerlinIn', 0.6804291009902954),\n",
       " ('Schöneberg', 0.6698859930038452)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec.most_similar('Berlin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vec_en = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hamburg', 0.7192949056625366),\n",
       " ('Leipzig', 0.7079555988311768),\n",
       " ('Germany', 0.7065216302871704),\n",
       " ('Bonn', 0.6962816715240479),\n",
       " ('Frankfurt', 0.6828957796096802),\n",
       " ('Munich', 0.6818758249282837),\n",
       " ('Dresden', 0.6666932106018066),\n",
       " ('Charlottenburg', 0.6547130346298218),\n",
       " ('Cologne', 0.6527722477912903),\n",
       " ('Potsdam', 0.6491367220878601)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec_en.most_similar('Berlin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 0.6976648569107056),\n",
       " ('Python-', 0.6554610729217529),\n",
       " ('wxPython', 0.6542694568634033),\n",
       " ('IronPython', 0.6238301396369934),\n",
       " ('Pythons', 0.6180740594863892),\n",
       " ('Jython', 0.6156101226806641),\n",
       " ('Monthy', 0.6128896474838257),\n",
       " ('Cython', 0.6123481392860413),\n",
       " ('Python-Programm', 0.6106103658676147),\n",
       " ('CPython', 0.5990972518920898)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec.most_similar('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 0.7034494876861572),\n",
       " ('Pythons', 0.7013207674026489),\n",
       " ('molurus', 0.6848306655883789),\n",
       " ('Stackless', 0.6760033369064331),\n",
       " ('Pygame', 0.66436767578125),\n",
       " ('Monthy', 0.6636004447937012),\n",
       " ('PyGTK', 0.635582447052002),\n",
       " ('NumPy', 0.6299797296524048),\n",
       " ('sebae', 0.628106415271759),\n",
       " ('CPython', 0.6107180118560791)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec_en.most_similar('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery = ft_vec['Berlin'] - ft_vec['Germany'] + ft_vec['France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7478695511817932),\n",
       " ('Paris', 0.6620789766311646),\n",
       " ('Berlin', 0.5965796709060669),\n",
       " ('Marseille', 0.5772451162338257),\n",
       " ('Paris-Brüssel', 0.57389235496521),\n",
       " ('Toulouse', 0.5723322033882141),\n",
       " ('Lyon', 0.5629856586456299),\n",
       " ('Marseille-Provence', 0.561870276927948),\n",
       " ('Chartre', 0.5531403422355652),\n",
       " ('Paris-Lyon', 0.5527874231338501)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec.most_similar([mystery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery = ft_vec_en['Berlin'] - ft_vec_en['Germany'] + ft_vec_en['France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris', 0.8141087293624878),\n",
       " ('Berlin', 0.7963954210281372),\n",
       " ('France', 0.7063857913017273),\n",
       " ('Brussels', 0.635625958442688),\n",
       " ('Cannes', 0.6236768960952759),\n",
       " ('Toulouse', 0.6210619807243347),\n",
       " ('Moscow', 0.6173150539398193),\n",
       " ('Marseille', 0.6148085594177246),\n",
       " ('Parisian', 0.6095399856567383),\n",
       " ('London', 0.6034213304519653)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec_en.most_similar([mystery])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut down the size of the embedding matrix to make loading the vectors a little easier\n",
    "\n",
    "- Can you think of a downside to this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spc = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_vocab = set(token.text for text in gnad_train.text for token in spc.tokenizer(text) if token.text in ft_vec)\n",
    "active_vocab = active_vocab | set(token.text for text in gnad_test.text for token in spc.tokenizer(text) if token.text in ft_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175121"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      ", -0.0354 -0.0088 -0.0180 0.0160 -0.0152 0.0560 0.0079 0.0567 -0.0614 -0.1713 0.0450 -0.0251 -0.1888 -0.0495 -0.0025 -0.0287 -0.1078 -0.0978 0.0220 0.2466 0.0199 -0.0657 -0.0058 0.0561 -0.1126 0.0272 -0.0957 -0.0187 -0.0424 0.2044 0.3801 -0.0560 -0.0257 0.0537 0.1034 0.0380 -0.0044 -0.1712 -0.0480 0.1109 -0.0271 0.0272 -0.0983 0.0479 0.0011 -0.1056 -0.0040 0.0161 -0.0501 -0.1308 -0.0694 0.0367 -0.0100 -0.0618 -0.0011 -0.0166 0.0058 0.0276 -0.0076 -0.1025 -0.0248 0.0318 -0.1475 0.0365 0.0396 0.0253 -0.0647 -0.0085 0.0012 -0.0321 -0.0545 0.2657 0.0860 0.0243 -0.0705 -0.0145 0.0237 -0.0478 -0.0031 -0.0084 0.0618 -0.0030 0.0060 0.1196 0.1298 0.0500 0.0181 -0.0286 -0.0655 -0.0006 0.1899 -0.2675 0.0189 0.0274 -0.0886 -0.0716 0.0348 -0.2869 -0.0287 -0.0350 0.0047 -0.0182 0.0868 0.1080 0.0842 -0.1328 0.0148 -0.0586 -0.0322 -0.0326 -0.0491 -0.0112 -0.0650 0.0060 -0.1827 0.0720 0.0571 -0.0769 0.0478 -0.0087 0.2527 0.0112 -0.0003 0.0226 0.0196 -0.0313 0.0401 -0.0268 -0.0404 0.0393 -0.0621 0.1305 -0.0126 0.0768 0.0251 0.0495 0.2509 -0.2401 0.0598 0.0598 0.0168 -1.4784 0.0797 -0.0936 0.0419 0.0961 -0.0930 -0.0723 -0.0206 0.0514 0.0337 0.2034 -0.0211 0.0063 -0.0493 -0.0322 0.0339 -0.1839 0.0375 0.2527 0.0133 -0.0317 0.0544 0.0337 0.0055 0.1082 0.0129 0.0060 0.1246 0.0102 0.0321 -0.0204 -0.0555 0.0842 -0.0364 -0.0203 0.0033 -0.0592 0.1161 0.0302 -0.0880 -0.0089 0.0241 -0.0647 -0.0738 -0.2447 -0.0563 -0.0493 -0.0403 0.0310 -0.0633 -0.0592 -0.0763 -0.0319 -0.0433 -0.0263 0.1970 0.0658 -0.0294 -0.1093 -0.0389 -0.1006 -0.0144 -0.0381 -0.0363 0.0437 0.0165 0.0631 0.0676 0.1051 0.0344 0.0239 0.0608 0.1226 0.0358 0.0535 -0.0295 -0.0570 0.0161 0.0457 0.0075 0.0442 -0.0038 0.0378 0.0409 0.0052 -0.0542 0.0323 0.0297 0.0014 0.1969 0.0701 -0.0134 -0.0416 0.1082 0.0061 0.0236 0.0552 -0.3050 0.0180 0.0734 -0.0328 0.4229 -0.1109 0.0459 0.0135 0.1276 0.0025 0.0621 0.2011 0.0811 -0.0688 0.2001 -0.0067 -0.0706 0.0256 0.0133 0.0162 0.0217 -0.0717 -0.0691 -0.0270 0.0982 0.0005 0.0262 0.0499 0.0591 -0.0780 0.2796 0.1134 0.0670 0.0038 0.1318 0.0780 -0.0176 0.0297 0.5797 0.0077 0.0837 0.0966 0.2333 -0.0155 0.3322 -0.0258 0.0062 0.0377 -0.0123 0.0433 0.0372 -0.0361 0.0181 -0.0157 -0.0535 -0.0330 0.1214 -0.0217 -0.0086 -0.0606 -0.1096 0.0739\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 ./cc.de.300.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft_vec.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip ./cc.de.300.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000000', '300']\n",
      "[',', '-0.0354', '-0.0088', '-0.0180', '0.0160', '-0.0152', '0.0560', '0.0079', '0.0567', '-0.0614', '-0.1713', '0.0450', '-0.0251', '-0.1888', '-0.0495', '-0.0025', '-0.0287', '-0.1078', '-0.0978', '0.0220', '0.2466', '0.0199', '-0.0657', '-0.0058', '0.0561', '-0.1126', '0.0272', '-0.0957', '-0.0187', '-0.0424', '0.2044', '0.3801', '-0.0560', '-0.0257', '0.0537', '0.1034', '0.0380', '-0.0044', '-0.1712', '-0.0480', '0.1109', '-0.0271', '0.0272', '-0.0983', '0.0479', '0.0011', '-0.1056', '-0.0040', '0.0161', '-0.0501', '-0.1308', '-0.0694', '0.0367', '-0.0100', '-0.0618', '-0.0011', '-0.0166', '0.0058', '0.0276', '-0.0076', '-0.1025', '-0.0248', '0.0318', '-0.1475', '0.0365', '0.0396', '0.0253', '-0.0647', '-0.0085', '0.0012', '-0.0321', '-0.0545', '0.2657', '0.0860', '0.0243', '-0.0705', '-0.0145', '0.0237', '-0.0478', '-0.0031', '-0.0084', '0.0618', '-0.0030', '0.0060', '0.1196', '0.1298', '0.0500', '0.0181', '-0.0286', '-0.0655', '-0.0006', '0.1899', '-0.2675', '0.0189', '0.0274', '-0.0886', '-0.0716', '0.0348', '-0.2869', '-0.0287', '-0.0350', '0.0047', '-0.0182', '0.0868', '0.1080', '0.0842', '-0.1328', '0.0148', '-0.0586', '-0.0322', '-0.0326', '-0.0491', '-0.0112', '-0.0650', '0.0060', '-0.1827', '0.0720', '0.0571', '-0.0769', '0.0478', '-0.0087', '0.2527', '0.0112', '-0.0003', '0.0226', '0.0196', '-0.0313', '0.0401', '-0.0268', '-0.0404', '0.0393', '-0.0621', '0.1305', '-0.0126', '0.0768', '0.0251', '0.0495', '0.2509', '-0.2401', '0.0598', '0.0598', '0.0168', '-1.4784', '0.0797', '-0.0936', '0.0419', '0.0961', '-0.0930', '-0.0723', '-0.0206', '0.0514', '0.0337', '0.2034', '-0.0211', '0.0063', '-0.0493', '-0.0322', '0.0339', '-0.1839', '0.0375', '0.2527', '0.0133', '-0.0317', '0.0544', '0.0337', '0.0055', '0.1082', '0.0129', '0.0060', '0.1246', '0.0102', '0.0321', '-0.0204', '-0.0555', '0.0842', '-0.0364', '-0.0203', '0.0033', '-0.0592', '0.1161', '0.0302', '-0.0880', '-0.0089', '0.0241', '-0.0647', '-0.0738', '-0.2447', '-0.0563', '-0.0493', '-0.0403', '0.0310', '-0.0633', '-0.0592', '-0.0763', '-0.0319', '-0.0433', '-0.0263', '0.1970', '0.0658', '-0.0294', '-0.1093', '-0.0389', '-0.1006', '-0.0144', '-0.0381', '-0.0363', '0.0437', '0.0165', '0.0631', '0.0676', '0.1051', '0.0344', '0.0239', '0.0608', '0.1226', '0.0358', '0.0535', '-0.0295', '-0.0570', '0.0161', '0.0457', '0.0075', '0.0442', '-0.0038', '0.0378', '0.0409', '0.0052', '-0.0542', '0.0323', '0.0297', '0.0014', '0.1969', '0.0701', '-0.0134', '-0.0416', '0.1082', '0.0061', '0.0236', '0.0552', '-0.3050', '0.0180', '0.0734', '-0.0328', '0.4229', '-0.1109', '0.0459', '0.0135', '0.1276', '0.0025', '0.0621', '0.2011', '0.0811', '-0.0688', '0.2001', '-0.0067', '-0.0706', '0.0256', '0.0133', '0.0162', '0.0217', '-0.0717', '-0.0691', '-0.0270', '0.0982', '0.0005', '0.0262', '0.0499', '0.0591', '-0.0780', '0.2796', '0.1134', '0.0670', '0.0038', '0.1318', '0.0780', '-0.0176', '0.0297', '0.5797', '0.0077', '0.0837', '0.0966', '0.2333', '-0.0155', '0.3322', '-0.0258', '0.0062', '0.0377', '-0.0123', '0.0433', '0.0372', '-0.0361', '0.0181', '-0.0157', '-0.0535', '-0.0330', '0.1214', '-0.0217', '-0.0086', '-0.0606', '-0.1096', '0.0739']\n",
      "['.', '-0.0257', '0.0093', '-0.0562', '-0.0464', '-0.0113', '-0.0215', '-0.0403', '0.0917', '-0.0192', '-0.1132', '0.0313', '-0.0240', '-0.3250', '0.0260', '-0.0175', '0.0009', '-0.2245', '-0.0489', '-0.0176', '-0.1190', '0.0138', '-0.0558', '-0.0369', '0.0717', '-0.1730', '-0.0270', '-0.0865', '0.0107', '-0.0887', '0.0217', '-0.0038', '-0.0781', '-0.0086', '0.0035', '0.0213', '0.0338', '-0.0534', '-0.1024', '-0.7544', '-0.3169', '0.0230', '0.0272', '0.0958', '0.0102', '-0.0341', '-0.0685', '0.0366', '-0.0165', '0.0657', '0.1475', '-0.0129', '-0.0409', '0.0179', '-0.0316', '0.0229', '-0.0958', '-0.0198', '-0.0019', '0.0071', '-0.4372', '-0.0396', '0.0295', '-0.1341', '-0.0441', '-0.0068', '-0.0455', '-0.1462', '0.1193', '-0.0026', '0.0582', '-0.0465', '0.0199', '0.0637', '0.0126', '0.0060', '0.0226', '0.0135', '0.0074', '-0.0844', '0.0696', '0.0074', '-0.0103', '-0.1021', '-0.0708', '-0.1286', '-0.0377', '-0.0363', '-0.0546', '-0.0501', '0.0563', '0.2338', '-0.4164', '-0.0452', '0.0764', '-0.1293', '-0.1070', '0.0658', '-0.0873', '-0.0465', '-0.0393', '-0.0366', '0.0091', '-0.0158', '0.0835', '-0.1021', '-0.0724', '0.0454', '0.0513', '0.0417', '0.0304', '0.0049', '-0.0455', '0.0749', '0.0711', '-0.0359', '0.0809', '-0.0415', '-0.0937', '-0.0267', '0.0218', '0.2319', '-0.0086', '0.0136', '-0.0151', '0.0271', '-0.0365', '0.0266', '0.0749', '-0.1532', '0.0128', '0.1715', '0.0766', '-0.0169', '0.0391', '0.0005', '0.0693', '-0.2092', '-0.0123', '-0.1145', '-0.0106', '0.0465', '-1.3823', '0.0723', '0.0895', '-0.0413', '0.0059', '-0.0635', '-0.0118', '0.0596', '0.1098', '0.0203', '0.1045', '0.0392', '-0.0543', '-0.1200', '-0.2033', '-0.0168', '-0.2086', '-0.1041', '0.4930', '0.0397', '0.0195', '-0.0206', '-0.0888', '-0.0464', '0.1280', '0.0196', '0.0083', '0.4939', '-0.0528', '-0.0615', '-0.0409', '0.0299', '-0.0969', '0.0066', '0.1038', '0.0090', '0.0505', '0.2883', '-0.0016', '0.0102', '-0.0774', '0.0103', '-0.0217', '0.0471', '-0.2707', '-0.0705', '-0.0519', '-0.0887', '0.0412', '-0.0739', '-0.0503', '-0.1271', '0.0205', '-0.0047', '-0.1598', '-0.0000', '0.0525', '0.0224', '-0.0768', '-0.1202', '-0.0384', '-0.0381', '-0.0044', '-0.0040', '-0.0391', '-0.0163', '-0.0065', '-0.0112', '0.0487', '0.0324', '0.0030', '0.0445', '-0.0165', '-0.0974', '0.0131', '0.0438', '0.0260', '0.0575', '0.0363', '0.0438', '-0.0428', '-0.0165', '0.0527', '-0.1299', '-0.0243', '-0.0607', '0.0096', '0.0918', '-0.0870', '-0.0692', '-0.0493', '0.0291', '0.0029', '0.1926', '-0.0014', '0.0492', '0.0262', '-0.3673', '-0.0536', '-0.0036', '0.0294', '-0.0531', '-0.3508', '-0.0190', '-0.0384', '0.0522', '-0.0386', '0.0294', '-0.2429', '0.0783', '0.0010', '0.1199', '-0.0546', '-0.0119', '-0.0676', '0.0197', '0.0084', '0.0469', '-0.0121', '-0.0052', '-0.0197', '-0.0234', '-0.0779', '0.1184', '-0.0144', '0.0832', '0.0120', '-0.1504', '0.0874', '0.0453', '-0.0377', '0.0233', '0.0299', '0.0821', '0.0699', '0.7157', '-0.0120', '0.0859', '0.0811', '0.3156', '0.1684', '0.8140', '0.0021', '0.0095', '-0.0293', '0.0640', '-0.0707', '0.0342', '-0.0977', '0.0285', '-0.0730', '-0.0512', '-0.1066', '-0.0717', '-0.0040', '0.0471', '0.0404', '-0.0495', '-0.0133']\n",
      "['</s>', '-0.0668', '-0.2122', '-0.2325', '-0.0733', '0.0092', '0.1659', '-0.0287', '-0.0430', '-0.0489', '-0.2975', '-0.1703', '-0.0143', '-0.4966', '0.0629', '-0.1116', '-0.0609', '-0.3640', '-0.0955', '-0.0644', '-0.0719', '-0.0158', '0.0281', '-0.0888', '0.0510', '-0.1517', '0.0289', '0.0315', '-0.0045', '-0.1007', '-0.0566', '0.1944', '-0.0132', '0.0488', '-0.0265', '-0.0497', '0.0097', '0.0039', '-0.2686', '-0.5113', '0.0939', '0.0334', '-0.0322', '0.0519', '0.0114', '0.0277', '-0.0636', '0.0129', '-0.0246', '0.0806', '0.1738', '-0.0497', '-0.0915', '0.0298', '-0.0337', '0.0535', '-0.1553', '-0.0413', '-0.0089', '-0.0238', '-0.4373', '-0.1409', '-0.0419', '-0.0671', '-0.0906', '0.0468', '0.0535', '-1.0696', '0.1396', '0.0072', '0.0750', '-0.0413', '0.2151', '0.0442', '-0.1342', '-0.0082', '-0.0115', '0.0262', '-0.0856', '-0.0369', '0.0071', '0.0206', '-0.1053', '0.0473', '-0.5439', '-0.2100', '0.1156', '-0.0088', '0.0176', '-0.0134', '0.0639', '0.2344', '-0.4076', '-0.0289', '-0.0945', '-0.0745', '0.0164', '0.0912', '-0.0600', '-0.0491', '-0.0509', '-0.0072', '0.0577', '0.0684', '0.0563', '-0.1688', '-0.1378', '-0.0738', '-0.0909', '0.0659', '0.0296', '0.0474', '-0.0740', '0.0709', '0.0722', '-0.0286', '-0.0060', '-0.0316', '0.0340', '-0.0646', '-0.1665', '0.4629', '0.0552', '-0.0213', '-0.0102', '0.0845', '-0.0448', '0.1592', '0.0312', '-0.0132', '0.0942', '0.2342', '-0.1261', '0.0199', '0.0962', '-0.0029', '-0.0012', '-0.0601', '-0.3025', '0.0005', '0.0069', '-0.0129', '-1.8111', '-0.0798', '0.0100', '-0.0133', '0.0084', '0.0004', '-0.1082', '-0.0803', '-0.0353', '-0.0633', '0.0592', '0.0352', '-0.0078', '0.0293', '-0.2693', '0.0311', '-0.4148', '0.0241', '0.2427', '-0.0338', '0.0374', '-0.0272', '0.0106', '-0.0017', '0.3320', '0.0162', '0.0199', '0.2508', '0.0566', '0.0080', '0.0080', '0.0407', '-0.0000', '0.0353', '0.0935', '0.0561', '0.0470', '0.1876', '-0.0102', '-0.0808', '-0.0267', '0.1112', '-0.0668', '0.1999', '-0.5382', '-0.0029', '-0.0334', '-0.1221', '0.0212', '-0.0120', '0.0081', '-0.0384', '0.0110', '0.0441', '-0.2519', '-0.3734', '0.0614', '-0.0607', '-0.0390', '-0.1294', '0.0154', '0.0712', '0.0424', '0.0174', '0.0564', '-0.0385', '-0.0158', '0.1061', '0.0079', '-0.0390', '-0.0283', '-0.0290', '-0.0799', '-0.0614', '0.0762', '0.1376', '0.0208', '0.0004', '-0.0255', '0.0461', '-0.0532', '-0.0611', '0.0144', '-0.0920', '-0.0112', '-0.0950', '0.0526', '0.0473', '-0.0045', '0.1451', '0.0284', '-0.0103', '-0.0574', '0.1513', '0.0175', '0.0396', '0.0250', '-0.1961', '0.0211', '-0.0268', '0.0242', '0.0346', '-0.3371', '-0.0000', '-0.1163', '0.2738', '0.0671', '0.0520', '0.0197', '0.3653', '-0.0475', '0.3523', '0.0383', '-0.0371', '-0.0137', '-0.0370', '0.0329', '0.0416', '0.0804', '-0.0188', '0.0870', '-0.0256', '-0.0519', '-0.0084', '-0.0278', '0.0556', '0.0653', '-0.0218', '0.0723', '-0.0336', '-0.0297', '-0.8277', '-0.1405', '-0.0017', '0.2645', '1.1159', '0.0324', '0.1174', '0.1102', '0.4196', '0.1441', '0.8478', '-0.0395', '0.0131', '0.0380', '-0.0258', '-0.0238', '0.0156', '-0.0004', '-0.0747', '0.0584', '-0.0295', '0.0261', '-0.0026', '0.0090', '0.0132', '-0.1163', '0.1194', '0.0202']\n",
      "['und', '-0.0715', '0.0249', '0.0993', '0.0060', '-0.0224', '-0.1758', '0.0039', '-0.0076', '0.0220', '-0.0541', '0.0860', '-0.0187', '-0.0846', '-0.0407', '0.0035', '-0.0192', '0.0918', '-0.0448', '-0.0400', '0.0550', '-0.0000', '-0.0708', '-0.0106', '0.0640', '-0.0519', '-0.0115', '-0.1030', '-0.0153', '-0.0053', '-0.0965', '0.0331', '-0.0345', '-0.0223', '-0.0709', '-0.0174', '0.0016', '-0.0142', '-0.0135', '0.1382', '-0.1077', '0.0022', '-0.0242', '-0.0301', '-0.0198', '-0.0389', '0.0848', '0.0087', '0.0274', '0.0028', '-0.1183', '-0.0088', '0.0337', '0.0312', '0.0157', '-0.0889', '0.0660', '0.0046', '0.0257', '-0.0418', '-0.0889', '-0.0387', '0.0281', '-0.0497', '0.0687', '-0.0072', '-0.0496', '0.0884', '-0.1751', '0.0436', '-0.0693', '0.0707', '0.0473', '0.0078', '0.0269', '0.0074', '0.0065', '-0.0261', '-0.0289', '-0.0114', '-0.0269', '0.0136', '-0.0119', '0.0160', '0.1015', '0.0022', '0.0819', '-0.0392', '-0.0556', '-0.0522', '0.0272', '0.1321', '0.1205', '0.0026', '0.0352', '-0.0453', '0.0443', '-0.0218', '-0.0461', '0.0923', '0.0141', '-0.0143', '0.0161', '0.0000', '0.0088', '0.0358', '-0.0453', '0.0413', '0.0337', '-0.0259', '0.0027', '0.0154', '0.0029', '0.0105', '0.0214', '-0.0297', '-0.0941', '-0.0119', '0.0204', '0.0827', '-0.1459', '-0.1000', '0.0044', '0.0259', '-0.0308', '-0.0156', '0.0118', '-0.0998', '-0.0041', '-0.0634', '-0.0550', '0.0205', '0.1600', '0.0024', '-0.0493', '0.0429', '0.0333', '0.0423', '-0.1632', '0.0524', '-0.0395', '0.0689', '-0.7409', '0.0481', '-0.0107', '0.0198', '0.0016', '-0.0289', '0.0619', '0.0150', '0.0938', '0.0233', '0.0592', '-0.0028', '-0.0997', '-0.0345', '0.0921', '-0.0423', '-0.0116', '-0.0373', '0.0381', '0.0131', '-0.0051', '-0.0159', '0.0048', '-0.0011', '0.0171', '0.0302', '0.0261', '-0.0416', '-0.0528', '-0.0173', '-0.0154', '0.0297', '-0.0318', '-0.0149', '0.0811', '0.0228', '0.0374', '0.0421', '-0.0172', '0.0205', '0.0303', '-0.0336', '-0.1165', '-0.0138', '0.0083', '0.0087', '0.0243', '0.0204', '0.0296', '-0.0343', '0.0422', '0.0064', '-0.0133', '-0.0246', '0.1057', '0.0739', '0.0570', '-0.0068', '0.0833', '0.0663', '0.0321', '-0.0394', '-0.0114', '-0.0264', '-0.0467', '0.0258', '0.0261', '-0.0519', '0.0054', '0.0150', '-0.0107', '0.0189', '0.0668', '-0.1150', '-0.0354', '-0.0396', '-0.0044', '0.0403', '-0.0301', '-0.0185', '-0.0019', '0.0057', '0.0089', '0.0540', '0.0123', '0.0273', '-0.0040', '0.0103', '-0.0226', '0.1346', '-0.0219', '-0.0234', '0.0155', '0.0674', '-0.0314', '-0.0194', '0.0139', '0.0425', '-0.0221', '0.0434', '0.0144', '0.0398', '-0.0533', '-0.0263', '0.0481', '-0.0129', '-0.0861', '-0.0162', '-0.0816', '0.0489', '0.0170', '-0.0317', '-0.0003', '0.0207', '0.0041', '-0.0982', '-0.0376', '-0.0408', '-0.0445', '0.0520', '-0.0007', '-0.0265', '0.0039', '-0.0042', '0.0029', '0.0192', '-0.0222', '0.1393', '0.0165', '0.0106', '-0.0413', '-0.2357', '0.0515', '0.0114', '-0.0679', '0.1245', '0.0395', '-0.1555', '-0.1216', '-0.0460', '-0.1452', '0.0507', '0.0145', '-0.0506', '0.0130', '0.0161', '-0.0820', '-0.0077', '-0.0035', '0.0191', '-0.0097', '-0.0342', '-0.0630', '0.1677', '-0.0496', '0.0199', '-0.0050', '0.0839', '-0.0595']\n",
      "['der', '0.0159', '0.0473', '0.1034', '0.0016', '0.0154', '0.3112', '-0.0258', '0.0002', '0.0087', '-0.1292', '0.0798', '-0.0009', '-0.1121', '-0.0276', '-0.0069', '-0.0009', '0.0887', '-0.0265', '-0.0015', '-0.1117', '-0.0010', '-0.0110', '-0.0098', '0.0381', '-0.0681', '-0.0184', '-0.0364', '-0.0150', '0.0005', '-0.0499', '-0.3041', '-0.0093', '-0.0279', '-0.0485', '-0.0252', '0.0031', '0.0072', '0.0782', '0.2211', '0.0167', '0.0180', '-0.0102', '0.0140', '-0.0408', '0.0024', '0.0420', '0.0242', '0.0001', '0.0011', '-0.2914', '0.0150', '-0.0485', '0.0509', '-0.0075', '-0.0418', '0.0745', '0.0025', '0.0131', '-0.0190', '0.1075', '-0.0413', '-0.0267', '-0.0562', '0.0214', '-0.0018', '-0.0184', '0.1344', '-0.0644', '0.0428', '0.0080', '0.0367', '0.0154', '-0.0137', '0.0847', '0.0142', '0.0023', '0.0057', '-0.0061', '-0.0475', '0.0316', '-0.0124', '-0.0243', '-0.0300', '0.1056', '-0.0801', '-0.1031', '-0.0342', '-0.0335', '0.0103', '0.0286', '0.3241', '0.1974', '-0.0646', '0.0508', '-0.0406', '0.0100', '0.0155', '0.0692', '0.4002', '0.0535', '0.0248', '0.0071', '-0.0047', '0.0308', '-0.0644', '0.0269', '0.0615', '0.0487', '-0.0218', '0.0331', '-0.0334', '0.0165', '0.0177', '-0.0321', '-0.0071', '0.0317', '-0.0014', '-0.0418', '0.0166', '-0.1210', '-0.0447', '0.0264', '-0.0144', '-0.0404', '-0.0315', '-0.0369', '-0.0145', '-0.0051', '-0.0199', '-0.0584', '-0.0038', '0.0474', '-0.0524', '0.0004', '-0.0024', '0.0488', '0.1671', '0.0934', '0.1182', '-0.0163', '-0.0072', '-0.5129', '-0.0165', '-0.0153', '0.0285', '-0.0175', '-0.0191', '0.0011', '0.0027', '0.0411', '0.0033', '0.1279', '-0.0050', '-0.1996', '-0.0071', '-0.0239', '-0.0528', '0.0061', '-0.0433', '-0.2084', '0.0366', '-0.0147', '-0.0283', '-0.0894', '-0.0145', '-0.0026', '0.0271', '-0.0202', '0.1543', '-0.0428', '0.0071', '-0.0173', '-0.0103', '-0.0345', '0.0157', '0.2463', '-0.0355', '-0.0106', '0.0605', '0.0193', '0.0562', '-0.0232', '-0.0046', '-0.0225', '-0.2013', '-0.0430', '0.0376', '0.0493', '0.0556', '0.0238', '-0.0131', '0.0310', '0.0226', '-0.0019', '0.0023', '-0.2142', '0.1395', '0.0552', '0.0113', '0.0463', '-0.0626', '0.0113', '0.0022', '0.0180', '0.0198', '-0.0029', '0.0417', '-0.0445', '-0.0179', '-0.0145', '0.0257', '-0.0030', '-0.0260', '0.0675', '0.0891', '-0.0184', '-0.1191', '0.0153', '0.0141', '-0.0123', '0.0193', '-0.0058', '0.0003', '-0.0046', '0.0519', '0.0082', '0.0254', '0.0420', '-0.0124', '-0.0225', '-0.0028', '0.0162', '-0.0037', '-0.0702', '0.0803', '0.0517', '-0.0192', '-0.0133', '0.5750', '-0.0044', '0.0518', '0.0110', '0.1352', '-0.1302', '-0.0261', '-0.0083', '-0.1054', '-0.0215', '-0.0449', '-0.0638', '-0.0032', '0.0321', '0.0369', '-0.0336', '0.0197', '-0.0320', '0.0124', '-0.0483', '0.0051', '0.0180', '-0.0127', '0.0190', '-0.0355', '-0.0140', '0.0769', '-0.0324', '0.1669', '-0.0076', '-0.0594', '-0.0866', '0.0059', '-0.0353', '-0.2671', '0.0561', '0.0256', '-0.0129', '0.1679', '-0.0036', '0.0740', '0.1227', '-0.1483', '0.0584', '-0.1894', '0.0304', '-0.0044', '-0.0149', '0.0266', '-0.0307', '0.0149', '0.0428', '-0.0233', '0.0109', '-0.0064', '-0.0161', '0.0467', '-0.0060', '0.0174', '0.0465', '0.0605', '-0.0628']\n",
      "[':', '-0.0392', '-0.1829', '-0.2038', '-0.0265', '0.0237', '0.1201', '0.0804', '-0.0434', '0.0759', '-0.1896', '-0.2453', '0.0792', '-0.2060', '0.0469', '-0.1609', '0.0191', '-0.0249', '0.0119', '-0.0246', '0.0350', '0.0230', '0.1952', '0.0631', '-0.0504', '-0.0399', '-0.0024', '-0.1692', '-0.0068', '-0.0316', '-0.0392', '0.2713', '0.0044', '0.0460', '-0.2637', '0.0636', '-0.0874', '-0.0409', '-0.1328', '-0.2966', '-0.1664', '0.0006', '-0.0571', '0.1406', '-0.0749', '-0.0347', '-0.2824', '0.0362', '-0.0799', '0.0761', '0.4295', '0.0482', '0.0083', '-0.1066', '-0.0011', '0.0635', '0.0363', '0.0230', '-0.0516', '0.0007', '-0.0386', '-0.0399', '-0.0598', '0.0739', '-0.0340', '0.0691', '0.0727', '0.1525', '-0.0140', '0.0504', '-0.1557', '0.0255', '0.1518', '0.0334', '-0.1257', '0.0089', '0.0361', '0.0708', '-0.0584', '0.0202', '-0.0114', '-0.0990', '-0.0050', '0.0066', '0.0557', '0.0136', '-0.2243', '0.0709', '0.0423', '0.0134', '-0.0123', '0.1629', '-0.3332', '0.0456', '-0.1799', '-0.0129', '0.0164', '-0.0408', '0.1752', '-0.1816', '0.1145', '0.0559', '0.0023', '0.0302', '-0.1041', '-0.3685', '-0.1460', '-0.0514', '-0.1424', '-0.0116', '-0.0862', '-0.0844', '0.0192', '-0.0049', '0.0133', '-0.0073', '0.1201', '0.0009', '-0.0852', '-0.0701', '0.2862', '0.0963', '0.1168', '-0.0282', '0.0837', '0.0855', '0.0227', '0.0645', '0.0537', '0.0017', '0.0274', '-0.0815', '0.1784', '0.1620', '0.1899', '0.0751', '-0.0721', '-0.1824', '0.0914', '-0.0542', '0.0820', '-0.1589', '-1.3505', '-0.0140', '0.0778', '-0.0803', '0.0488', '0.0170', '0.0763', '-0.0642', '-0.1175', '-0.0208', '0.1231', '-0.0523', '0.1940', '0.0277', '-0.1668', '-0.0053', '-0.2197', '0.0832', '0.2361', '-0.0523', '0.0253', '0.0765', '0.0372', '-0.1332', '0.1987', '0.0035', '0.0431', '-0.0830', '-0.0364', '0.0782', '-0.0571', '-0.0262', '-0.0222', '0.0982', '0.0147', '0.1138', '0.0833', '-0.1367', '0.0146', '0.0070', '-0.0487', '0.2477', '-0.1159', '0.0167', '-0.2781', '0.2763', '-0.0323', '-0.0792', '-0.0098', '-0.0536', '-0.1192', '0.0509', '0.0690', '-0.0022', '-0.1827', '0.1290', '0.0458', '0.0786', '-0.0436', '-0.2332', '-0.0099', '0.0030', '-0.0568', '0.0428', '-0.0666', '-0.0966', '-0.0252', '0.1099', '0.0573', '0.0705', '0.0482', '0.0333', '-0.1509', '-0.1470', '0.1688', '0.1024', '0.0055', '0.0196', '-0.0846', '-0.0429', '0.1023', '0.0248', '0.0417', '-0.0790', '-0.0587', '-0.0850', '0.0366', '0.0562', '0.0144', '0.0683', '-0.0243', '-0.0854', '-0.0442', '-0.1732', '-0.0428', '0.0560', '0.0482', '-0.0220', '0.0124', '-0.0745', '0.0203', '0.1661', '-0.1582', '0.0109', '-0.0099', '0.1410', '-0.0193', '0.0189', '-0.3169', '0.1490', '-0.0186', '-0.2045', '-0.0160', '-0.0385', '0.0137', '0.0020', '0.0606', '0.0770', '0.0388', '-0.0567', '-0.0341', '-0.0775', '-0.0102', '0.0586', '-0.0698', '0.0146', '-0.0623', '-0.1224', '0.3707', '-0.0070', '0.0517', '-0.0544', '-0.0722', '-0.1020', '0.2640', '0.6759', '-0.0312', '-0.0816', '0.3125', '0.4243', '-0.2102', '0.2546', '0.0409', '0.0705', '0.0003', '-0.0213', '-0.3155', '0.1156', '0.0275', '-0.0743', '-0.0307', '-0.0281', '0.0546', '-0.1684', '-0.0313', '0.0268', '-0.1611', '-0.1354', '0.0988']\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('./cc.de.300.vec.gz', 'rb') as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        print(line.decode('utf8').split())\n",
    "        if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open('./cc.de.300.vec.SMALL.gz', 'wb') as fh:\n",
    "    fh.write(f'{len(active_vocab)} {300}\\n'.encode('utf8'))\n",
    "    for w in list(active_vocab):\n",
    "        vec = ft_vec[w]\n",
    "        vec_str = ' '.join(f'{itm:.4f}' for itm in vec)\n",
    "        fh.write(f'{w} {vec_str}\\n'.encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -c ./cc.de.300.vec.SMALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate(ft_vec.index2word)}\n",
    "vectors = ft_vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "active_vocab = list(active_vocab)\n",
    "active_vectors = np.asarray([ft_vec[v] for v in active_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_vectors.shape\n",
    "index2word = active_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vec.vectors = active_vectors\n",
    "ft_vec.index2word = active_vocab\n",
    "ft_vec.index2entity = active_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x144049410>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('braunem', 0.698927640914917),\n",
       " ('weißem', 0.6904536485671997),\n",
       " ('bemaltem', 0.6705213785171509),\n",
       " ('gefärbtes', 0.6649290323257446),\n",
       " ('schwarzem', 0.6638764142990112),\n",
       " ('hergestelltem', 0.6627174615859985),\n",
       " ('milchigem', 0.6624775528907776),\n",
       " ('gefärbten', 0.6580451130867004),\n",
       " ('angereichertem', 0.6528030633926392),\n",
       " ('rotem', 0.6521549224853516)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vec.most_similar('Berlin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Load some data (20 newsgroups) and turn it into something that can be fed into a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_train = fetch_20newsgroups(subset='train', data_home='./data', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc'], array([7, 4, 4]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['target_names'][:3], news['target'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an index for fords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f9003cc24ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word2idx = dict((w, idx) for (idx, w) in enumerate(model.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2918"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['Berlin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>[0.1073, 0.0089, 0.0006, 0.0055, -0.0646, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>[0.0495, 0.0411, 0.0041, 0.0309, -0.0044, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>)</td>\n",
       "      <td>[-0.0675, 0.0383, -0.0183, 0.0028, -0.0074, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>on</td>\n",
       "      <td>[0.0206, 0.0231, -0.0574, 0.0388, -0.1158, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The</td>\n",
       "      <td>[0.0797, -0.0294, 0.0033, 0.0458, 0.0174, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>by</td>\n",
       "      <td>[0.0242, -0.0265, 0.0822, -0.0322, 0.0042, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>you</td>\n",
       "      <td>[-0.1202, 0.07, 0.103, 0.0568, 0.1457, -0.1787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "      <td>[-0.0092, -0.0478, -0.038, 0.0125, 0.0175, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>;</td>\n",
       "      <td>[0.125, -0.0241, -0.0904, 0.0432, 0.1913, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>their</td>\n",
       "      <td>[0.0145, -0.0674, -0.0034, 0.0964, -0.0182, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>said</td>\n",
       "      <td>[0.0552, -0.0181, -0.0561, 0.0086, -0.0203, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>will</td>\n",
       "      <td>[-0.0368, 0.0231, -0.1271, -0.1208, -0.033, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>also</td>\n",
       "      <td>[-0.0764, 0.0451, -0.0883, 0.0223, -0.2671, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>people</td>\n",
       "      <td>[0.0104, 0.1101, -0.0982, -0.0106, -0.0696, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>they</td>\n",
       "      <td>[0.0793, 0.0045, 0.0034, 0.0801, -0.0688, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>#</td>\n",
       "      <td>[-0.0133, 0.0093, -0.023, 0.0666, 0.0421, 0.16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                             vector\n",
       "idx                                                           \n",
       "0         ,  [0.1073, 0.0089, 0.0006, 0.0055, -0.0646, -0.0...\n",
       "1        to  [0.0495, 0.0411, 0.0041, 0.0309, -0.0044, -0.1...\n",
       "2         )  [-0.0675, 0.0383, -0.0183, 0.0028, -0.0074, -0...\n",
       "3        on  [0.0206, 0.0231, -0.0574, 0.0388, -0.1158, -0....\n",
       "4       The  [0.0797, -0.0294, 0.0033, 0.0458, 0.0174, -0.0...\n",
       "5        by  [0.0242, -0.0265, 0.0822, -0.0322, 0.0042, -0....\n",
       "6       you  [-0.1202, 0.07, 0.103, 0.0568, 0.1457, -0.1787...\n",
       "7         -  [-0.0092, -0.0478, -0.038, 0.0125, 0.0175, -0....\n",
       "8         ;  [0.125, -0.0241, -0.0904, 0.0432, 0.1913, -0.0...\n",
       "9     their  [0.0145, -0.0674, -0.0034, 0.0964, -0.0182, 0....\n",
       "10     said  [0.0552, -0.0181, -0.0561, 0.0086, -0.0203, -0...\n",
       "11     will  [-0.0368, 0.0231, -0.1271, -0.1208, -0.033, -0...\n",
       "12     also  [-0.0764, 0.0451, -0.0883, 0.0223, -0.2671, 0....\n",
       "13   people  [0.0104, 0.1101, -0.0982, -0.0106, -0.0696, 0....\n",
       "14     they  [0.0793, 0.0045, 0.0034, 0.0801, -0.0688, -0.1...\n",
       "15        #  [-0.0133, 0.0093, -0.023, 0.0666, 0.0421, 0.16..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame([(idx, word, ft_vec[word])\n",
    "              for idx, word in enumerate(ft_vec.index2word[:80:5])],\n",
    "             columns=['idx', 'word', 'vector']).set_index('idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we set the vector for out-of-vocabulary words to the mean of all other vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_vector = model.vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend the feed forward network from before by adding an embedding layer\n",
    "\n",
    "The embedding layer is just a lookup table that maps a word index to a vector. The embeddings can be pre-trained embeddings, like fastText or word2vec or glove, or you can learn them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class FeedForwardWithEmbeddings(nn.Module):\n",
    "    def __init__(self, layers, pretrained_embeddings=None,\n",
    "                 num_embeddings=None, embedding_dim=50,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if pretrained_embeddings:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "            self.embedding_dim = embedding.size(1)\n",
    "            self.num_embeddings = embedding.size(0)\n",
    "        elif num_embeddings is not None and num_embeddings > 0 and embedding_dim > 0:\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        else:\n",
    "            raise Exception('You must either define pretrained embeddings or both the number and dimensionality of embeddings.')\n",
    "       \n",
    "        \n",
    "        # create feed forward layers\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_features=in_features, out_features=out_features)\n",
    "                                     for (in_features, out_features) in layers])\n",
    "        \n",
    "        # TODO: initialise weights\n",
    "        \n",
    "        # create activation functions for each layer\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, labels=None):\n",
    "        output = None\n",
    "        X = self.embedding(X)\n",
    "        for i_layer, layer in enumerate(self.layers):\n",
    "            if output is None:\n",
    "                output = self.activation(layer(X))\n",
    "            else:\n",
    "                output = self.activation(layer(output))\n",
    "        # do softmax\n",
    "        prob = F.log_softmax(output, dim=1).exp()\n",
    "        return prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-00e737533b92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize(msg):\n",
    "    tokens = re.findall('\\w+', msg)\n",
    "    vectors = [word2idx[t] if t in ft_vec else unk_vector\n",
    "               for t in tokens][:100]\n",
    "    if len(vectors) < 100:\n",
    "        vectors += [unk_vector] * (100 - len(vectors))\n",
    "    return vectors\n",
    "\n",
    "def map_label(label_idx):\n",
    "    return news['target_names'][label_idx].split('.')[0]\n",
    "\n",
    "X, y = zip(*[(tokenize(x), map_label(y_idx)) for x, y_idx in zip(news['data'], news['target'])])\n",
    "label_map = {label: idx for idx, label in enumerate(set(y))}\n",
    "y = [label_map[y_] for y_ in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: The precomputed fast text word embeddings contain a lot more entries than those in the 20 newsgroups data set. For practical purposes one _could_ leave only those words that are contained in the training data. This, however, comes at the cost of potentially losing information for future unseen words that _are_ in the pretrained fast text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler, DataLoader, TensorDataset\n",
    "\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "data = TensorDataset(X, y)\n",
    "sampler = RandomSampler(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeedFor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a55395df370f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedFor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdmn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeedFor' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdmn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "ffn = FeedForwardWithEmbeddings(ft_vec.vectors)\n",
    "optimizer = \n",
    "scheduler = \n",
    "\n",
    "for _ in tqdmn(range(num_epochs), total=num_epochs, desc=\"Epoch\"):\n",
    "    steps = tqdmn(train_dataloader,\n",
    "                  total=X.size(0) // (train_dataloader.batch_size + 1),\n",
    "                  desc='Mini-batch')\n",
    "    train_loss = 0\n",
    "    for i_step, batch in enumerate(steps):\n",
    "        batch_X, batch_y = batch\n",
    "        loss, *_ = ffn(batch_X, labels=batch_y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(bert.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Bag of Tricks for Efficient Text Classification\n",
    "- [Edouard Grave, Tomas Mikolov, Armand Joulin, Piotr Bojanowski: Bag of Tricks for Efficient Text Classification. EACL (2) 2017: 427-431](https://www.aclweb.org/anthology/E17-2068/)\n",
    "\n",
    "\n",
    "| Model | AG | Sogou | DBP | Yelp P. | Yelp F. | Yah. A. | Amz. F. | Amz. P. |\n",
    "| ----- | -- | ----- | --- | ------- | ------- | ------- | ------- | ------- |\n",
    "| BoW (Zhang et al., 2015) | 88.8 | 92.9 | 96.6 | 92.2 | 58.0 | 68.9 | 54.6 | 90.4 |\n",
    "| ngrams (Zhang et al., 2015) | 92.0 | 97.1 | 98.6 | 95.6 | 56.3 | 68.5 | 54.3 | 92.0 |\n",
    "| ngrams TFIDF (Zhang et al., 2015) | 92.4 | 97.2 | 98.7 | 95.4 | 54.8 | 68.5 | 52.4 | 91.5 |\n",
    "| char-CNN (Zhang and LeCun, 2015) | 87.2 | 95.1 | 98.3 | 94.7 | 62.0 | 71.2 | 59.5 | 94.5 |\n",
    "| char-CRNN (Xiao and Cho, 2016) | 91.4 | 95.2 | 98.6 | 94.5 | 61.8 | 71.7 | 59.2 | 94.1 |\n",
    "| VDCNN (Conneau et al., 2016) | 91.3 | 96.8 | 98.7 | 95.7 | 64.7 | 73.4 | 63.0 | 95.7 |\n",
    "| |\n",
    "| fastText, h = 10 | 91.5 | 93.9 | 98.1 | 93.8 | 60.4 | 72.0 | 55.8 | 91.2 |\n",
    "| fastText, h = 10, bigram | 92.5 | 96.8 | 98.6 | 95.7 | 63.9 | 72.3 | 60.2 | 94.6 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
