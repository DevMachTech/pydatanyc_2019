{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "- Chris Olah [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Chris Olah, Shan Carter [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns)\n",
    "- Andreas Madsen [Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns)\n",
    "- Chris Nicholson [A Beginner's Guide to LSTMs and Recurrent Neural Networks](https://skymind.ai/wiki/lstm)\n",
    "- Michael Nguyen [Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "- Michael Nguyen [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "---\n",
    "\n",
    "- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)\n",
    "  > \"_For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL._\"\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Natural language does not usually come in neatly packaged fixed length sequences. The simple feed forward neural network from part 1, however, assumed that we can cut the documents down to 100 words. For some applications and data sources this may be approriate for others not. For instance, cutting tweets down to 100 or 200 words would not be that unreasonable, but cutting wikipedia articles would.\n",
    "\n",
    "Recurrent neural networks get around, to some extent, this limitation on the sequence length. Given infinite time they can consume an infinite sequence. The network maintains an internal state that keeps track of what is important and what isn't. There are [lots of different kinds of recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) the main difference between being the way in which the internal state is updated and maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415900486749594"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage of the 20 newsgroups documents are below our document length threshold?\n",
    "sum([True for d in gnad_train.text if len(d.split()) > 100]) / len(gnad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Österreich bleibt in der EM-Qualifikation für 2016 ungeschlagen, ja, feiert in und gegen Russland den sechsten Auswärtssieg en suite. Marc Janko macht mit einem Traum von einem Tor die Reise nach Frankreich beinahe zur Gewissheit. Moskau - Sie wollten unmittelbar vor dem Urlaub die allerletzten Kräfte mobilisieren, im abschließenden Saisonspiel einen Kampf bis zum Umfallen abliefern, den Russen zeigen, wer Tabellenführer der Gruppe G ist. Und die österreichische Fußballnationalmannschaft mobilisierte, zeigte. Umgefallen ist sie nicht. Teamchef Marcel Koller konnte, vom verletzten David Alaba abgesehen (wurde durch Stefan Ilsanker ersetzt), aus dem Vollen schöpfen. Martin Harnik ist fit geworden, also lief die Einsergarnitur ein. Der Boss hatte den Auftrag erteilt, hinten kompakt zu stehen, aber in erster Linie Fußball zu spielen. Und sie spielten von Anpfiff an. Russland kam gar nicht zum Schauen, fand keine Zeit, eine Ordnung zu finden. Die Gäste pressten, drückten, kombinierten. Sie waren geistig voll auf der Höhe und konzentriert, kombinierten sich durch die nicht ausverkaufte Otkrytije Arena. Die  linke Seite mit Kapitän Christian Fuchs und Marko Arnautovic wirbelte in der Moskauer Schwüle, schon bald  war klar, dass Zlatko Junuzovic der absolute Wahnsinn ist. Er schaltete seinen Motor ein, das gesamte Feld wurde sein Arbeitsplatz, der Bremen-Legionär war Passgeber, Antreiber, Schaltzentrale, Herz, Lunge, Hirn. Jankos Bewerbung In der vierten Minute wurde Russlands Torhüter Igor Akinfejew noch von einem eigenen Verteidiger geprüft, danach sorgten die Österreicher selbst für Gefahr. 7. Minute: Arnautovic misslingt der allerletzte Pass ganz knapp. Zehnte Minute: Outeinwurf von Fuchs in die Tiefe, Junuzovic, wer sonst, scheitert. Russland wirkte gehemmt, überrascht, perplex. Trainer Fabio Capello stand wie versteinert an der Seitenlinie. Nur eine gute Aktion hat er von den Seinen schon gesehen, Robert Almer parierte einen Schuss von Oleg Schatow (28.). Aus Capellos Sicht war das Gegentor allerdings nur eine Frage der Zeit, in der 33. Minute ist es gefallen. Junuzovic, von Harnik bedient, scheitert binnen Sekunden zweimal an Akinfejew, die Verteidigung bringt den Ball nicht aus dem Gefahrenbereich. Gefahr wird im österreichischen Spitzenfußball bekanntlich durch Marc Janko personifiziert. Der 31-Jährige hebt ab, legt sich quer, mit dem Rücken zum Tor. Es war der Beginn eines großartigen Fallrückziehers, der im 1:0 endete. Adjektive wie fulminant, famos, traumhaft für sein 21. Länderspieltor (46 Einsätze) sind schamlose Untertreibungen. Jubel. Das war eines der schöneren und wichtigeren Tore meine Karriere, ich hoffe, es war nicht das letzte, sollte Janko danach sagen. Der Mann sucht übrigens einen Verein. Russland im Schock, mit dieser Sanktion war nicht zu rechnen. 35. Minute: Junuzovic spielt Julian Baumgartlinger frei, der läuft ohne Verfolger gen Tor, schießt daneben. Das 2:0 wäre die Entscheidung gewesen. Es muss schon sehr blöd hergehen Nach der Pause änderte sich das Bild doch ein bisserl drastisch. Die Russen wollten nicht als Versager dastehen, legten zu. Almer war urplötzlich nicht mehr unterbeschäftigt, die Innenverteidiger Aleksandar Dragovic und Martin Hinteregger wurden mit einer Art Walze konfrontiert. Die Österreicher hatten Teilchen ihrer Aggressivität verloren. Der erschöpfte Harnik wurde durch Marcel Sabitzer ersetzt (65.), Torschütze Janko machte Platz für Rubin Okotie (75.). Der knappe Vorsprung wurde dank einer fast schon kitschigen Aufopferung  über die Runden gebracht. Da haben die Jungs alles rausgehaut, sagte Coach Marcel Koller. Die Teilnahme an der EM-Endrunde 2016 in Frankreich ist kaum noch zu verhindern. ÖFB-Präsident Leo Windtner sprach:_Wir haben den TGV Richtung Frankreich in Bewegung gesetzt. Russland, zuvor 21 Heimspiele en suite unbesiegt, wird eher zuschauen müssen. Nochmals Janko: Jetzt muss es schon sehr blöd hergehen, dass wir das aus der Hand geben. Österreichs Lustreise zum Wiedersehen mit Frankreich nach der WM-Teilnahme 1998 wird am 5. September in Wien gegen die Republik Moldau fortgesetzt, drei Tage später werden in Stockholm die Schweden beehrt. Aber jetzt ist Urlaub. Wohlverdient. (Christian Hackl aus Moskau, 14.6.2015) Fußball-EM-Qualifikation, Gruppe G/6. Runde: Russland - Österreich 0:1 (0:1). Moskau, Otkrytije Arena, 35.000, SR Milorad Mazic (SRB) Tor: 0:1 (33.) Janko Russland: Akinfejew - Smolnikow, Nowoselzew, Beresuzki (12. Tschernow), Kombarow (72. Kerschakow) - Gluschakow, Iwanow (46. Mirantschuk) - Schatow, Schirokow, Schirkow - Kokorin Österreich: Almer - Klein, Dragovic, Hinteregger, Fuchs - Baumgartlinger, Ilsanker - Harnik (65. Sabitzer), Junuzovic (87. Prödl), Arnautovic - Janko (76. Okotie) Gelbe Karten: Kokorin bzw. Klein\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_articles = [idx for idx, d in enumerate(gnad_train.text) if len(d.split()) > 100]\n",
    "print(gnad_train.text[long_articles[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Idea Behind RNNs\n",
    "\n",
    "![](1280px-Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "\n",
    "<sub>Image By François Deloche, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elman Network\n",
    "\n",
    "$$ h_t = \\sigma{(W_hx_t + U_hh_{t-1}+b_h)} $$\n",
    "$$ y_t = \\sigma{(W_yh_t + b_y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Elman Network in Practice\n",
    "\n",
    "PyTorch has an implementation of an Elman network as the base `nn.RNN` class. The class enables uni- or bi-directional training and supports an arbitrary number of network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "ft_vec = KeyedVectors.load_word2vec_format('./cc.de.300.vec.SMALL.gz')\n",
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.concatenate([np.random.rand(1, 300), ft_vec.vectors])\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "rnn = nn.RNN(ft_vec.vectors.shape[1], 10, num_layers=1)\n",
    "\n",
    "UNK = np.random.rand(1, ft_vec.vectors[0].shape[0])\n",
    "vectors = np.concatenate([UNK, ft_vec.vectors], axis=0)\n",
    "sent = 'Die neue BER wird bald geöffnet'.split()\n",
    "word_idx = [word2idx[w] for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175121"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[88478, 91678, 142125, 84236, 251, 38088]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0138, -0.0134, -0.0165,  ..., -0.0256,  0.0201, -0.0439],\n",
       "        [-0.1180, -0.0394, -0.0214,  ...,  0.0711,  0.0084, -0.0139],\n",
       "        [ 0.0351, -0.0030,  0.0232,  ...,  0.0440, -0.0155, -0.0054],\n",
       "        [-0.0824,  0.1310, -0.1151,  ...,  0.0324,  0.0836,  0.0936],\n",
       "        [-0.0111,  0.0363, -0.0078,  ..., -0.0070,  0.0040, -0.0265],\n",
       "        [ 0.0368, -0.0124,  0.0299,  ...,  0.0051,  0.0385, -0.0078]])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can feed individual documents through the embedding layer ...\n",
    "emb(torch.LongTensor(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and then feed the embedded document through the RNN\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1940, -0.0908, -0.1706,  0.3068,  0.4633, -0.4665,  0.5681,\n",
       "           0.4381,  0.3606,  0.0855],\n",
       "         [-0.3440,  0.0154, -0.2777,  0.1304,  0.3534, -0.5114,  0.4618,\n",
       "           0.5423,  0.4515,  0.1751],\n",
       "         [-0.1124,  0.0558, -0.1972,  0.3448,  0.3629, -0.5432,  0.5532,\n",
       "           0.5761,  0.4233,  0.0113],\n",
       "         [-0.2124,  0.2233,  0.1864,  0.4236,  0.0920, -0.4740,  0.2988,\n",
       "           0.5289,  0.1059,  0.1892],\n",
       "         [-0.3647,  0.4744, -0.2364,  0.4440,  0.5096, -0.4413,  0.5552,\n",
       "           0.5197,  0.3503, -0.1133],\n",
       "         [-0.1695,  0.0051, -0.1974,  0.2503,  0.4625, -0.4975,  0.5649,\n",
       "           0.6053,  0.3775,  0.0334]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0699,  0.4773, -0.2269,  0.0485,  0.3940, -0.6445, -0.1033,\n",
       "          -0.4411, -0.2875, -0.1820],\n",
       "         [ 0.4298,  0.4310,  0.2294,  0.3335, -0.0151, -0.5991, -0.5415,\n",
       "          -0.2417, -0.4159, -0.5094],\n",
       "         [-0.1932,  0.1373, -0.6091,  0.3863,  0.1393, -0.2915, -0.2880,\n",
       "          -0.3617, -0.8717, -0.1275],\n",
       "         [ 0.5143,  0.7247,  0.0362,  0.5913, -0.2482, -0.6949,  0.3882,\n",
       "          -0.2663,  0.0628,  0.2348],\n",
       "         [-0.0294,  0.0504, -0.2885, -0.1780,  0.2908, -0.3357, -0.2692,\n",
       "          -0.0353,  0.0394, -0.2128],\n",
       "         [ 0.3471,  0.2983, -0.0493,  0.2738,  0.3065, -0.1331, -0.0619,\n",
       "          -0.6883, -0.6351, -0.3907],\n",
       "         [-0.2459,  0.0986, -0.3187, -0.2063,  0.3427,  0.1184, -0.2180,\n",
       "          -0.1361, -0.6476, -0.2689]],\n",
       "\n",
       "        [[-0.0255, -0.2716, -0.1416,  0.3043,  0.2569, -0.1661, -0.1401,\n",
       "          -0.3326,  0.1700, -0.0462],\n",
       "         [-0.3487, -0.4301,  0.0090,  0.1152,  0.2997, -0.0407, -0.2095,\n",
       "          -0.6039,  0.1502,  0.0897],\n",
       "         [-0.2930, -0.4659, -0.0934,  0.3153,  0.0370, -0.0437, -0.4436,\n",
       "          -0.1195,  0.3053,  0.1824],\n",
       "         [ 0.1636, -0.4788, -0.2220,  0.2902,  0.2477, -0.3367, -0.2781,\n",
       "          -0.3178,  0.3171, -0.1148],\n",
       "         [-0.0530, -0.2067, -0.3568,  0.1635,  0.0507, -0.0873, -0.1997,\n",
       "          -0.2115,  0.3918, -0.3242],\n",
       "         [-0.1261, -0.4157,  0.2901,  0.3289,  0.2442, -0.1014, -0.3587,\n",
       "          -0.4723,  0.2314,  0.2114],\n",
       "         [-0.1883, -0.2007, -0.0660,  0.1878, -0.0409,  0.0802, -0.2443,\n",
       "          -0.0300,  0.2753, -0.0333]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Actually,'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c9f36311606a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-c9f36311606a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Actually,'"
     ]
    }
   ],
   "source": [
    "UNK = torch.FloatTensor().random_()\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(ft_vec.vectors))\n",
    "sent = news_train.data[long_articles[12]].split()\n",
    "word_idx = [word2idx[w] for w in sent]\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load german word embeddings from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "ft_vec_DE = KeyedVectors.load_word2vec_format('./cc.de.300.vec.SMALL.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec_DE.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_train = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_train.text]\n",
    "y_train = label_encoder.fit_transform(gnad_train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "UNK = np.random.rand(1, 300)\n",
    "vectors = np.concatenate([UNK, ft_vec_DE.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A strange training loop (??)\n",
    "\n",
    "Let's train an LSTM classifier that uses that last hidden state from each sequence (document) as the representation for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.01 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = False\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=1, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 / 144 7.87s 1001.00s 2.2111\n",
      "iter 2 / 144 15.06s 994.00s 2.2263\n",
      "iter 3 / 144 21.86s 987.00s 2.2287\n",
      "iter 4 / 144 27.39s 840.00s 2.2228\n",
      "iter 5 / 144 33.10s 834.00s 2.2213\n",
      "iter 6 / 144 39.65s 828.00s 2.2114\n",
      "iter 7 / 144 45.28s 822.00s 2.2038\n",
      "iter 8 / 144 52.30s 816.00s 2.2002\n",
      "iter 9 / 144 60.54s 810.00s 2.1944\n",
      "iter 10 / 144 66.95s 804.00s 2.1845\n",
      "iter 11 / 144 73.32s 798.00s 2.1809\n",
      "iter 12 / 144 80.35s 792.00s 2.1778\n",
      "iter 13 / 144 87.38s 786.00s 2.1684\n",
      "iter 14 / 144 94.31s 780.00s 2.1664\n",
      "iter 15 / 144 101.28s 774.00s 2.1591\n",
      "iter 16 / 144 108.15s 768.00s 2.1529\n",
      "iter 17 / 144 115.09s 762.00s 2.1449\n",
      "iter 18 / 144 121.91s 756.00s 2.1424\n",
      "iter 19 / 144 129.18s 750.00s 2.1438\n",
      "iter 20 / 144 135.88s 744.00s 2.1431\n",
      "iter 21 / 144 142.04s 738.00s 2.1436\n",
      "iter 22 / 144 148.78s 732.00s 2.1435\n",
      "iter 23 / 144 155.89s 726.00s 2.1443\n",
      "iter 24 / 144 161.91s 720.00s 2.1411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-a19656122a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "h, c = None, None\n",
    "lossfct = nn.CrossEntropyLoss()\n",
    "for _ in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    start_time = time()\n",
    "    for i_step, (X_, y_) in enumerate(zip(X_train, y_train), 1):\n",
    "        X_ = torch.LongTensor(X_)\n",
    "        y_ = torch.LongTensor([y_])\n",
    "        \n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_).unsqueeze(dim=1)\n",
    "        output, *_ = lstm(embed)\n",
    "        output = classifier(output[-1, :, :])\n",
    "        loss = lossfct(output, y_)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        if i_step > 0 and i_step % 64 == 0:\n",
    "            delta = time() - start_time\n",
    "            avg_delta = delta // (i_step // 64)\n",
    "            clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            n_total = len(X_train) // 64\n",
    "            n_remaining = n_total - (i_step // 64)\n",
    "            print('iter', i_step // 64, '/', n_total, f'{delta:.2f}s', f'{avg_delta*n_remaining:.2f}s', f'{train_loss / i_step:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop above goes through one document at a time, this is very inefficient and unlikely to yield good performance. However, bacthing together variable length sequences requires a little bit of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_train]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_train), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_train):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_train))\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[132813, 130692,  82087,  ...,      0,      0,      0],\n",
       "         [     0,  85372, 104301,  ...,      0,      0,      0],\n",
       "         [ 28850, 148805,  88260,  ...,      0,      0,      0],\n",
       "         [     0,      0, 143159,  ...,      0,      0,      0],\n",
       "         [104810,  27421, 144802,  ...,      0,      0,      0]]),\n",
       " tensor([ 63, 578, 277,  97, 619]),\n",
       " tensor([5, 3, 6, 7, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 638, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "X_tmp, len_tmp, _ = dataset[:4]\n",
    "embed = emb(X_tmp)\n",
    "packed = pack_padded_sequence(embed, len_tmp, enforce_sorted=False, batch_first=True)\n",
    "output, *_ = lstm(packed)\n",
    "X, _ = pad_packed_sequence(output, batch_first=True)\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = False\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=1, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer_parameters = list(lstm.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.Adam(optimizer_parameters, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 / 288 5.17s 1485.12s 0.0352\n",
      "iter 2 / 288 25.02s 3578.53s -0.2289\n",
      "iter 3 / 288 33.82s 3213.04s -0.3485\n",
      "iter 4 / 288 42.62s 3026.32s -0.4296\n",
      "iter 5 / 288 47.86s 2709.02s -0.4726\n",
      "iter 6 / 288 50.90s 2392.28s -0.5477\n",
      "iter 7 / 288 67.01s 2690.09s -0.6141\n",
      "iter 8 / 288 82.43s 2884.93s -0.6444\n",
      "iter 9 / 288 88.52s 2744.11s -0.6836\n",
      "iter 10 / 288 94.82s 2635.93s -0.7015\n",
      "iter 11 / 288 98.72s 2485.83s -0.7364\n",
      "iter 12 / 288 106.03s 2438.67s -0.7642\n",
      "iter 13 / 288 111.30s 2354.38s -0.7782\n",
      "iter 14 / 288 116.12s 2272.60s -0.8028\n",
      "iter 15 / 288 167.51s 3048.64s -0.8134\n",
      "iter 16 / 288 178.71s 3038.00s -0.8206\n",
      "iter 17 / 288 187.69s 2992.07s -0.8328\n",
      "iter 18 / 288 194.72s 2920.83s -0.8480\n",
      "iter 19 / 288 207.42s 2936.67s -0.8632\n",
      "iter 20 / 288 211.08s 2828.52s -0.8830\n",
      "iter 21 / 288 218.55s 2778.72s -0.9011\n",
      "iter 22 / 288 222.93s 2695.44s -0.9170\n",
      "iter 23 / 288 225.96s 2603.43s -0.9268\n",
      "iter 24 / 288 234.33s 2577.63s -0.9322\n",
      "iter 25 / 288 239.64s 2520.98s -0.9403\n",
      "iter 26 / 288 245.59s 2474.75s -0.9501\n",
      "iter 27 / 288 250.59s 2422.40s -0.9564\n",
      "iter 28 / 288 270.77s 2514.30s -0.9727\n",
      "iter 29 / 288 278.99s 2491.63s -0.9844\n",
      "iter 30 / 288 286.74s 2465.93s -0.9966\n",
      "iter 31 / 288 294.18s 2438.81s -1.0116\n",
      "iter 32 / 288 301.82s 2414.58s -1.0203\n",
      "iter 33 / 288 302.24s 2335.52s -1.0396\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from time import time\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-2)\n",
    "\n",
    "num_epochs = 1\n",
    "lossfct = nn.NLLLoss()\n",
    "for _ in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    start_time = time()\n",
    "    for i_step, (X_batch, lengths, y_batch) in enumerate(dataloader, 1):\n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_batch)\n",
    "        \n",
    "        # run the embeddings through the LSTM\n",
    "        packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "        output, *_ = lstm(packed)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # run the encoded last hidden state through the classifier\n",
    "        last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "        output = classifier(last_hidden_states)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        loss = lossfct(output, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        delta = time() - start_time\n",
    "        avg_delta = delta / i_step\n",
    "        clip_grad_norm_(optimizer_parameters, 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        n_total = len(X_train) // dataloader.batch_size\n",
    "        n_remaining = n_total - i_step\n",
    "        print('iter', i_step, '/', n_total, f'{delta:.2f}s', f'{avg_delta*n_remaining:.2f}s', f'{train_loss / i_step:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3528, 1.5024, 1.6414, 1.2185, 0.4896, 1.8219, 1.4599, 2.0353, 1.7341],\n",
       "        [0.2771, 1.6668, 1.4966, 1.3643, 0.5157, 1.8477, 1.5806, 1.8347, 1.7740],\n",
       "        [0.3511, 1.5061, 1.6381, 1.2217, 0.4902, 1.8225, 1.4627, 2.0308, 1.7350],\n",
       "        [0.3546, 1.4984, 1.6449, 1.2149, 0.4889, 1.8212, 1.4570, 2.0402, 1.7331]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3063, 32, 300])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.permute(1, 0, 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_test = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_test.text]\n",
    "y_test = label_encoder.transform(gnad_test.category)\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_test]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_test), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_test):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_test))\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm.to(DEVICE)\n",
    "pred = []\n",
    "for i_step, (X_batch, lengths, y_batch) in enumerate(dataloader, 1):\n",
    "    X_batch = X_batch.to(DEVICE)\n",
    "    # run the word indices through the embedding layer and then the LSTM\n",
    "    embed = emb(X_batch)\n",
    "\n",
    "    # run the embeddings through the LSTM\n",
    "    packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "    output, *_ = lstm(packed)\n",
    "    output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    # run the encoded last hidden state through the classifier\n",
    "    last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "    output = classifier(last_hidden_states)\n",
    "\n",
    "    _, pred_ = F.log_softmax(output, dim=-1).max(dim=-1)\n",
    "    pred.extend(pred_.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Etat       0.00      0.00      0.00        67\n",
      "       Inland       0.00      0.00      0.00       102\n",
      "International       0.00      0.00      0.00       151\n",
      "       Kultur       0.00      0.00      0.00        54\n",
      "     Panorama       0.00      0.00      0.00       168\n",
      "        Sport       0.24      0.18      0.21       120\n",
      "          Web       0.00      0.00      0.00       168\n",
      "   Wirtschaft       0.14      0.94      0.25       141\n",
      " Wissenschaft       0.00      0.00      0.00        57\n",
      "\n",
      "     accuracy                           0.15      1028\n",
      "    macro avg       0.04      0.13      0.05      1028\n",
      " weighted avg       0.05      0.15      0.06      1028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder / Decoder architecture and Transfer Learning\n",
    "\n",
    "- Howard et al.: [_Universal Language Model Fine-tuning for Text Classification._](https://www.aclweb.org/anthology/P18-1031/) ACL (1) 2018: 328-339\n",
    "- Jeremy Howard and Sebastian Ruder [Introducing state of the art text classification with universal language models](http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) 15 May 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "lstm_stack = nn.LSTM(ft_vec.vectors[0].shape[0], 64, num_layers=3, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sport</td>\n",
       "      <td>21-Jähriger fällt wohl bis Saisonende aus. Wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>'Erfundene Bilder zu Filmen, die als verloren ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Web</td>\n",
       "      <td>Der frischgekürte CEO Sundar Pichai setzt auf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Putin: \"Einigung, dass wir Menge auf Niveau vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Estland sieht den künftigen österreichischen P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Bernd Saurer war Bridge-Juniorenweltmeister un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9241</td>\n",
       "      <td>International</td>\n",
       "      <td>Sandhere soll in vergangener Woche bei Luftang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9242</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Derzeit Konzeptgruppe in Berlin – Kein Komment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9243</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Landeshauptmann will den vierten Regierungssit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9244</td>\n",
       "      <td>Panorama</td>\n",
       "      <td>Er ist einer von Millionen syrischen Flüchtlin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9245 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0             Sport  21-Jähriger fällt wohl bis Saisonende aus. Wie...\n",
       "1            Kultur  'Erfundene Bilder zu Filmen, die als verloren ...\n",
       "2               Web  Der frischgekürte CEO Sundar Pichai setzt auf ...\n",
       "3        Wirtschaft  Putin: \"Einigung, dass wir Menge auf Niveau vo...\n",
       "4            Inland  Estland sieht den künftigen österreichischen P...\n",
       "...             ...                                                ...\n",
       "9240         Inland  Bernd Saurer war Bridge-Juniorenweltmeister un...\n",
       "9241  International  Sandhere soll in vergangener Woche bei Luftang...\n",
       "9242     Wirtschaft  Derzeit Konzeptgruppe in Berlin – Kein Komment...\n",
       "9243         Inland  Landeshauptmann will den vierten Regierungssit...\n",
       "9244       Panorama  Er ist einer von Millionen syrischen Flüchtlin...\n",
       "\n",
       "[9245 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnad_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_X_data = [word2idx[w] for sent in gnad_train.data for w in sent.split()]\n",
    "lm_y_data = lm_X_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "data = TensorDataset(X, y)\n",
    "sampler = RandomSampler(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained AWD-LSTM from fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  440M  100  440M    0     0  8935k      0  0:00:50  0:00:50 --:--:-- 9368k00:58  0:00:15  0:00:43 8796k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/fwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  440M  100  440M    0     0  6619k      0  0:01:08  0:01:08 --:--:-- 6592k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/fwd_wt103_enc.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4063k  100 4063k    0     0  2457k      0  0:00:01  0:00:01 --:--:-- 2456k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/itos_wt103.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import text\n",
    "\n",
    "# learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('itos_wt103.pkl', 'rb') as fh:\n",
    "    lm_vocab = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238462"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "gnad_train, _ = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnad_train.to_csv('lm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sport</td>\n",
       "      <td>21-Jähriger fällt wohl bis Saisonende aus. Wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>'Erfundene Bilder zu Filmen, die als verloren ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Web</td>\n",
       "      <td>Der frischgekürte CEO Sundar Pichai setzt auf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Putin: \"Einigung, dass wir Menge auf Niveau vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Estland sieht den künftigen österreichischen P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                               text\n",
       "0       Sport  21-Jähriger fällt wohl bis Saisonende aus. Wie...\n",
       "1      Kultur  'Erfundene Bilder zu Filmen, die als verloren ...\n",
       "2         Web  Der frischgekürte CEO Sundar Pichai setzt auf ...\n",
       "3  Wirtschaft  Putin: \"Einigung, dass wir Menge auf Niveau vo...\n",
       "4      Inland  Estland sieht den künftigen österreichischen P..."
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnad_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai.datasets.untar_data(url: str, fname: Union[pathlib.Path, str] = None, dest: Union[pathlib.Path, str] = None, data=True, force_download=False) -> pathlib.Path>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "path = fastai.datasets.untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/mattilyra/.fastai/data/imdb_sample')"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = text.data.TextLMDataBunch.from_csv(path, 'texts.csv')\n",
    "awd_lstm = text.language_model_learner(data_lm, text.AWD_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck? What made this the xxunk successful triumph it was ? Was it casting , music , imagination , ingenuity , or luck ? No , no , no , no , no , no , no , no ! No , no , no , no'"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm.beam_search('What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck?',\n",
    "                     n_words=25,\n",
    "                     temperature=5.95,\n",
    "                     top_k=25,\n",
    "                     beam_sz=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
