{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "- Chris Olah [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Chris Olah, Shan Carter [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns)\n",
    "- Andreas Madsen [Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns)\n",
    "- Chris Nicholson [A Beginner's Guide to LSTMs and Recurrent Neural Networks](https://skymind.ai/wiki/lstm)\n",
    "- Michael Nguyen [Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "- Michael Nguyen [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "---\n",
    "\n",
    "- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)\n",
    "  > \"_For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL._\"\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Natural language does not usually come in neatly packaged fixed length sequences. The simple feed forward neural network from part 1, however, assumed that we can cut the documents down to 100 words. For some applications and data sources this may be approriate for others not. For instance, cutting tweets down to 100 or 200 words would not be that unreasonable, but cutting wikipedia articles would.\n",
    "\n",
    "Recurrent neural networks get around, to some extent, this limitation on the sequence length. Given infinite time they can consume an infinite sequence. The network maintains an internal state that keeps track of what is important and what isn't. There are [lots of different kinds of recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) the main difference between being the way in which the internal state is updated and maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415900486749594"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage of the 20 newsgroups documents are below our document length threshold?\n",
    "sum([True for d in gnad_train.text if len(d.split()) > 100]) / len(gnad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Österreich bleibt in der EM-Qualifikation für 2016 ungeschlagen, ja, feiert in und gegen Russland den sechsten Auswärtssieg en suite. Marc Janko macht mit einem Traum von einem Tor die Reise nach Frankreich beinahe zur Gewissheit. Moskau - Sie wollten unmittelbar vor dem Urlaub die allerletzten Kräfte mobilisieren, im abschließenden Saisonspiel einen Kampf bis zum Umfallen abliefern, den Russen zeigen, wer Tabellenführer der Gruppe G ist. Und die österreichische Fußballnationalmannschaft mobilisierte, zeigte. Umgefallen ist sie nicht. Teamchef Marcel Koller konnte, vom verletzten David Alaba abgesehen (wurde durch Stefan Ilsanker ersetzt), aus dem Vollen schöpfen. Martin Harnik ist fit geworden, also lief die Einsergarnitur ein. Der Boss hatte den Auftrag erteilt, hinten kompakt zu stehen, aber in erster Linie Fußball zu spielen. Und sie spielten von Anpfiff an. Russland kam gar nicht zum Schauen, fand keine Zeit, eine Ordnung zu finden. Die Gäste pressten, drückten, kombinierten. Sie waren geistig voll auf der Höhe und konzentriert, kombinierten sich durch die nicht ausverkaufte Otkrytije Arena. Die  linke Seite mit Kapitän Christian Fuchs und Marko Arnautovic wirbelte in der Moskauer Schwüle, schon bald  war klar, dass Zlatko Junuzovic der absolute Wahnsinn ist. Er schaltete seinen Motor ein, das gesamte Feld wurde sein Arbeitsplatz, der Bremen-Legionär war Passgeber, Antreiber, Schaltzentrale, Herz, Lunge, Hirn. Jankos Bewerbung In der vierten Minute wurde Russlands Torhüter Igor Akinfejew noch von einem eigenen Verteidiger geprüft, danach sorgten die Österreicher selbst für Gefahr. 7. Minute: Arnautovic misslingt der allerletzte Pass ganz knapp. Zehnte Minute: Outeinwurf von Fuchs in die Tiefe, Junuzovic, wer sonst, scheitert. Russland wirkte gehemmt, überrascht, perplex. Trainer Fabio Capello stand wie versteinert an der Seitenlinie. Nur eine gute Aktion hat er von den Seinen schon gesehen, Robert Almer parierte einen Schuss von Oleg Schatow (28.). Aus Capellos Sicht war das Gegentor allerdings nur eine Frage der Zeit, in der 33. Minute ist es gefallen. Junuzovic, von Harnik bedient, scheitert binnen Sekunden zweimal an Akinfejew, die Verteidigung bringt den Ball nicht aus dem Gefahrenbereich. Gefahr wird im österreichischen Spitzenfußball bekanntlich durch Marc Janko personifiziert. Der 31-Jährige hebt ab, legt sich quer, mit dem Rücken zum Tor. Es war der Beginn eines großartigen Fallrückziehers, der im 1:0 endete. Adjektive wie fulminant, famos, traumhaft für sein 21. Länderspieltor (46 Einsätze) sind schamlose Untertreibungen. Jubel. Das war eines der schöneren und wichtigeren Tore meine Karriere, ich hoffe, es war nicht das letzte, sollte Janko danach sagen. Der Mann sucht übrigens einen Verein. Russland im Schock, mit dieser Sanktion war nicht zu rechnen. 35. Minute: Junuzovic spielt Julian Baumgartlinger frei, der läuft ohne Verfolger gen Tor, schießt daneben. Das 2:0 wäre die Entscheidung gewesen. Es muss schon sehr blöd hergehen Nach der Pause änderte sich das Bild doch ein bisserl drastisch. Die Russen wollten nicht als Versager dastehen, legten zu. Almer war urplötzlich nicht mehr unterbeschäftigt, die Innenverteidiger Aleksandar Dragovic und Martin Hinteregger wurden mit einer Art Walze konfrontiert. Die Österreicher hatten Teilchen ihrer Aggressivität verloren. Der erschöpfte Harnik wurde durch Marcel Sabitzer ersetzt (65.), Torschütze Janko machte Platz für Rubin Okotie (75.). Der knappe Vorsprung wurde dank einer fast schon kitschigen Aufopferung  über die Runden gebracht. Da haben die Jungs alles rausgehaut, sagte Coach Marcel Koller. Die Teilnahme an der EM-Endrunde 2016 in Frankreich ist kaum noch zu verhindern. ÖFB-Präsident Leo Windtner sprach:_Wir haben den TGV Richtung Frankreich in Bewegung gesetzt. Russland, zuvor 21 Heimspiele en suite unbesiegt, wird eher zuschauen müssen. Nochmals Janko: Jetzt muss es schon sehr blöd hergehen, dass wir das aus der Hand geben. Österreichs Lustreise zum Wiedersehen mit Frankreich nach der WM-Teilnahme 1998 wird am 5. September in Wien gegen die Republik Moldau fortgesetzt, drei Tage später werden in Stockholm die Schweden beehrt. Aber jetzt ist Urlaub. Wohlverdient. (Christian Hackl aus Moskau, 14.6.2015) Fußball-EM-Qualifikation, Gruppe G/6. Runde: Russland - Österreich 0:1 (0:1). Moskau, Otkrytije Arena, 35.000, SR Milorad Mazic (SRB) Tor: 0:1 (33.) Janko Russland: Akinfejew - Smolnikow, Nowoselzew, Beresuzki (12. Tschernow), Kombarow (72. Kerschakow) - Gluschakow, Iwanow (46. Mirantschuk) - Schatow, Schirokow, Schirkow - Kokorin Österreich: Almer - Klein, Dragovic, Hinteregger, Fuchs - Baumgartlinger, Ilsanker - Harnik (65. Sabitzer), Junuzovic (87. Prödl), Arnautovic - Janko (76. Okotie) Gelbe Karten: Kokorin bzw. Klein\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_articles = [idx for idx, d in enumerate(gnad_train.text) if len(d.split()) > 100]\n",
    "print(gnad_train.text[long_articles[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Idea Behind RNNs\n",
    "\n",
    "![](1280px-Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "\n",
    "<sub>Image By François Deloche, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elman Network\n",
    "\n",
    "$$ h_t = \\sigma{(W_hx_t + U_hh_{t-1}+b_h)} $$\n",
    "$$ y_t = \\sigma{(W_yh_t + b_y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Elman Network in Practice\n",
    "\n",
    "PyTorch has an implementation of an Elman network as the base `nn.RNN` class. The class enables uni- or bi-directional training and supports an arbitrary number of network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "ft_vec = KeyedVectors.load_word2vec_format('./cc.de.300.vec.SMALL.gz')\n",
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.concatenate([np.random.rand(1, 300), ft_vec.vectors])\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "rnn = nn.RNN(ft_vec.vectors.shape[1], 10, num_layers=1)\n",
    "\n",
    "UNK = np.random.rand(1, ft_vec.vectors[0].shape[0])\n",
    "vectors = np.concatenate([UNK, ft_vec.vectors], axis=0)\n",
    "sent = 'Die neue BER wird bald geöffnet'.split()\n",
    "word_idx = [word2idx[w] for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175121"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[88478, 91678, 142125, 84236, 251, 38088]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0138, -0.0134, -0.0165,  ..., -0.0256,  0.0201, -0.0439],\n",
       "        [-0.1180, -0.0394, -0.0214,  ...,  0.0711,  0.0084, -0.0139],\n",
       "        [ 0.0351, -0.0030,  0.0232,  ...,  0.0440, -0.0155, -0.0054],\n",
       "        [-0.0824,  0.1310, -0.1151,  ...,  0.0324,  0.0836,  0.0936],\n",
       "        [-0.0111,  0.0363, -0.0078,  ..., -0.0070,  0.0040, -0.0265],\n",
       "        [ 0.0368, -0.0124,  0.0299,  ...,  0.0051,  0.0385, -0.0078]])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can feed individual documents through the embedding layer ...\n",
    "emb(torch.LongTensor(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and then feed the embedded document through the RNN\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1940, -0.0908, -0.1706,  0.3068,  0.4633, -0.4665,  0.5681,\n",
       "           0.4381,  0.3606,  0.0855],\n",
       "         [-0.3440,  0.0154, -0.2777,  0.1304,  0.3534, -0.5114,  0.4618,\n",
       "           0.5423,  0.4515,  0.1751],\n",
       "         [-0.1124,  0.0558, -0.1972,  0.3448,  0.3629, -0.5432,  0.5532,\n",
       "           0.5761,  0.4233,  0.0113],\n",
       "         [-0.2124,  0.2233,  0.1864,  0.4236,  0.0920, -0.4740,  0.2988,\n",
       "           0.5289,  0.1059,  0.1892],\n",
       "         [-0.3647,  0.4744, -0.2364,  0.4440,  0.5096, -0.4413,  0.5552,\n",
       "           0.5197,  0.3503, -0.1133],\n",
       "         [-0.1695,  0.0051, -0.1974,  0.2503,  0.4625, -0.4975,  0.5649,\n",
       "           0.6053,  0.3775,  0.0334]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0699,  0.4773, -0.2269,  0.0485,  0.3940, -0.6445, -0.1033,\n",
       "          -0.4411, -0.2875, -0.1820],\n",
       "         [ 0.4298,  0.4310,  0.2294,  0.3335, -0.0151, -0.5991, -0.5415,\n",
       "          -0.2417, -0.4159, -0.5094],\n",
       "         [-0.1932,  0.1373, -0.6091,  0.3863,  0.1393, -0.2915, -0.2880,\n",
       "          -0.3617, -0.8717, -0.1275],\n",
       "         [ 0.5143,  0.7247,  0.0362,  0.5913, -0.2482, -0.6949,  0.3882,\n",
       "          -0.2663,  0.0628,  0.2348],\n",
       "         [-0.0294,  0.0504, -0.2885, -0.1780,  0.2908, -0.3357, -0.2692,\n",
       "          -0.0353,  0.0394, -0.2128],\n",
       "         [ 0.3471,  0.2983, -0.0493,  0.2738,  0.3065, -0.1331, -0.0619,\n",
       "          -0.6883, -0.6351, -0.3907],\n",
       "         [-0.2459,  0.0986, -0.3187, -0.2063,  0.3427,  0.1184, -0.2180,\n",
       "          -0.1361, -0.6476, -0.2689]],\n",
       "\n",
       "        [[-0.0255, -0.2716, -0.1416,  0.3043,  0.2569, -0.1661, -0.1401,\n",
       "          -0.3326,  0.1700, -0.0462],\n",
       "         [-0.3487, -0.4301,  0.0090,  0.1152,  0.2997, -0.0407, -0.2095,\n",
       "          -0.6039,  0.1502,  0.0897],\n",
       "         [-0.2930, -0.4659, -0.0934,  0.3153,  0.0370, -0.0437, -0.4436,\n",
       "          -0.1195,  0.3053,  0.1824],\n",
       "         [ 0.1636, -0.4788, -0.2220,  0.2902,  0.2477, -0.3367, -0.2781,\n",
       "          -0.3178,  0.3171, -0.1148],\n",
       "         [-0.0530, -0.2067, -0.3568,  0.1635,  0.0507, -0.0873, -0.1997,\n",
       "          -0.2115,  0.3918, -0.3242],\n",
       "         [-0.1261, -0.4157,  0.2901,  0.3289,  0.2442, -0.1014, -0.3587,\n",
       "          -0.4723,  0.2314,  0.2114],\n",
       "         [-0.1883, -0.2007, -0.0660,  0.1878, -0.0409,  0.0802, -0.2443,\n",
       "          -0.0300,  0.2753, -0.0333]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Actually,'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c9f36311606a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-c9f36311606a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Actually,'"
     ]
    }
   ],
   "source": [
    "UNK = torch.FloatTensor().random_()\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(ft_vec.vectors))\n",
    "sent = news_train.data[long_articles[12]].split()\n",
    "word_idx = [word2idx[w] for w in sent]\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load german word embeddings from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "ft_vec_DE = KeyedVectors.load_word2vec_format('./cc.de.300.vec.SMALL.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec_DE.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_train = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_train.text]\n",
    "y_train = label_encoder.fit_transform(gnad_train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "UNK = np.random.rand(1, 300)\n",
    "vectors = np.concatenate([UNK, ft_vec_DE.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A strange training loop (??)\n",
    "\n",
    "Let's train an LSTM classifier that uses that last hidden state from each sequence (document) as the representation for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.01 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = False\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=1, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 / 144 7.87s 1001.00s 2.2111\n",
      "iter 2 / 144 15.06s 994.00s 2.2263\n",
      "iter 3 / 144 21.86s 987.00s 2.2287\n",
      "iter 4 / 144 27.39s 840.00s 2.2228\n",
      "iter 5 / 144 33.10s 834.00s 2.2213\n",
      "iter 6 / 144 39.65s 828.00s 2.2114\n",
      "iter 7 / 144 45.28s 822.00s 2.2038\n",
      "iter 8 / 144 52.30s 816.00s 2.2002\n",
      "iter 9 / 144 60.54s 810.00s 2.1944\n",
      "iter 10 / 144 66.95s 804.00s 2.1845\n",
      "iter 11 / 144 73.32s 798.00s 2.1809\n",
      "iter 12 / 144 80.35s 792.00s 2.1778\n",
      "iter 13 / 144 87.38s 786.00s 2.1684\n",
      "iter 14 / 144 94.31s 780.00s 2.1664\n",
      "iter 15 / 144 101.28s 774.00s 2.1591\n",
      "iter 16 / 144 108.15s 768.00s 2.1529\n",
      "iter 17 / 144 115.09s 762.00s 2.1449\n",
      "iter 18 / 144 121.91s 756.00s 2.1424\n",
      "iter 19 / 144 129.18s 750.00s 2.1438\n",
      "iter 20 / 144 135.88s 744.00s 2.1431\n",
      "iter 21 / 144 142.04s 738.00s 2.1436\n",
      "iter 22 / 144 148.78s 732.00s 2.1435\n",
      "iter 23 / 144 155.89s 726.00s 2.1443\n",
      "iter 24 / 144 161.91s 720.00s 2.1411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-a19656122a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "h, c = None, None\n",
    "lossfct = nn.CrossEntropyLoss()\n",
    "for _ in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    start_time = time()\n",
    "    for i_step, (X_, y_) in enumerate(zip(X_train, y_train), 1):\n",
    "        X_ = torch.LongTensor(X_)\n",
    "        y_ = torch.LongTensor([y_])\n",
    "        \n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_).unsqueeze(dim=1)\n",
    "        output, *_ = lstm(embed)\n",
    "        output = classifier(output[-1, :, :])\n",
    "        loss = lossfct(output, y_)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        if i_step > 0 and i_step % 64 == 0:\n",
    "            delta = time() - start_time\n",
    "            avg_delta = delta // (i_step // 64)\n",
    "            clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            n_total = len(X_train) // 64\n",
    "            n_remaining = n_total - (i_step // 64)\n",
    "            print('iter', i_step // 64, '/', n_total, f'{delta:.2f}s', f'{avg_delta*n_remaining:.2f}s', f'{train_loss / i_step:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop above goes through one document at a time, this is very inefficient and unlikely to yield good performance. However, bacthing together variable length sequences requires a little bit of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_train]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_train), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_train):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_train))\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 638, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "X_tmp, len_tmp, _ = dataset[:4]\n",
    "embed = emb(X_tmp)\n",
    "packed = pack_padded_sequence(embed, len_tmp, enforce_sorted=False, batch_first=True)\n",
    "output, *_ = lstm(packed)\n",
    "X, _ = pad_packed_sequence(output, batch_first=True)\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = False\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 128, num_layers=1, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer_parameters = list(lstm.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.SGD(optimizer_parameters, lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9245"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 / 288 8.98s 2576.51s 2.1942\n",
      "iter 2 / 288 39.77s 5687.70s 2.2196\n",
      "iter 3 / 288 77.70s 7381.58s 2.2010\n",
      "iter 4 / 288 87.96s 6245.00s 2.2061\n",
      "iter 5 / 288 99.88s 5653.16s 2.1862\n",
      "iter 6 / 288 113.34s 5326.85s 2.1868\n",
      "iter 7 / 288 126.84s 5091.52s 2.1921\n",
      "iter 8 / 288 137.35s 4807.21s 2.2007\n",
      "iter 9 / 288 144.30s 4473.41s 2.1939\n",
      "iter 10 / 288 155.36s 4319.06s 2.1971\n",
      "iter 11 / 288 171.68s 4323.12s 2.1981\n",
      "iter 12 / 288 193.72s 4455.50s 2.1928\n",
      "iter 13 / 288 202.85s 4291.16s 2.1881\n",
      "iter 14 / 288 211.43s 4137.98s 2.1839\n",
      "iter 15 / 288 234.40s 4266.05s 2.1850\n",
      "iter 16 / 288 242.83s 4128.14s 2.1824\n",
      "iter 17 / 288 273.50s 4359.97s 2.1858\n",
      "iter 18 / 288 295.30s 4429.53s 2.1832\n",
      "iter 19 / 288 305.45s 4324.46s 2.1764\n",
      "iter 20 / 288 321.83s 4312.51s 2.1748\n",
      "iter 21 / 288 332.41s 4226.32s 2.1743\n",
      "iter 22 / 288 340.00s 4110.89s 2.1723\n",
      "iter 23 / 288 375.60s 4327.60s 2.1749\n",
      "iter 24 / 288 385.42s 4239.64s 2.1757\n",
      "iter 25 / 288 427.76s 4500.08s 2.1739\n",
      "iter 26 / 288 436.05s 4394.02s 2.1689\n",
      "iter 27 / 288 454.25s 4391.11s 2.1674\n",
      "iter 28 / 288 472.73s 4389.66s 2.1658\n",
      "iter 29 / 288 491.97s 4393.79s 2.1646\n",
      "iter 30 / 288 509.17s 4378.84s 2.1625\n",
      "iter 31 / 288 530.20s 4395.56s 2.1602\n",
      "iter 32 / 288 554.08s 4432.67s 2.1562\n",
      "iter 33 / 288 571.79s 4418.35s 2.1548\n",
      "iter 34 / 288 579.10s 4326.22s 2.1519\n",
      "iter 35 / 288 589.41s 4260.56s 2.1530\n",
      "iter 36 / 288 603.50s 4224.53s 2.1506\n",
      "iter 37 / 288 624.64s 4237.44s 2.1512\n",
      "iter 38 / 288 640.78s 4215.67s 2.1523\n",
      "iter 39 / 288 655.19s 4183.14s 2.1527\n",
      "iter 40 / 288 671.52s 4163.43s 2.1538\n",
      "iter 41 / 288 693.14s 4175.75s 2.1521\n",
      "iter 42 / 288 714.94s 4187.53s 2.1539\n",
      "iter 43 / 288 724.75s 4129.40s 2.1546\n",
      "iter 44 / 288 756.28s 4193.91s 2.1549\n",
      "iter 45 / 288 764.17s 4126.50s 2.1541\n",
      "iter 46 / 288 782.57s 4117.01s 2.1548\n",
      "iter 47 / 288 798.76s 4095.75s 2.1534\n",
      "iter 48 / 288 806.59s 4032.94s 2.1525\n",
      "iter 49 / 288 832.81s 4062.07s 2.1526\n",
      "iter 50 / 288 844.54s 4020.00s 2.1520\n",
      "iter 51 / 288 859.27s 3993.07s 2.1516\n",
      "iter 52 / 288 870.41s 3950.32s 2.1514\n",
      "iter 53 / 288 886.12s 3929.02s 2.1508\n",
      "iter 54 / 288 892.74s 3868.54s 2.1492\n",
      "iter 55 / 288 909.37s 3852.43s 2.1492\n",
      "iter 56 / 288 918.04s 3803.30s 2.1467\n",
      "iter 57 / 288 929.29s 3766.05s 2.1468\n",
      "iter 58 / 288 951.22s 3772.06s 2.1480\n",
      "iter 59 / 288 968.21s 3757.97s 2.1494\n",
      "iter 60 / 288 977.77s 3715.53s 2.1499\n",
      "iter 61 / 288 986.98s 3672.87s 2.1493\n",
      "iter 62 / 288 1000.24s 3646.05s 2.1501\n",
      "iter 63 / 288 1009.97s 3607.03s 2.1492\n",
      "iter 64 / 288 1026.06s 3591.21s 2.1483\n",
      "iter 65 / 288 1032.29s 3541.56s 2.1475\n",
      "iter 66 / 288 1046.03s 3518.48s 2.1482\n",
      "iter 67 / 288 1054.64s 3478.73s 2.1461\n",
      "iter 68 / 288 1073.23s 3472.21s 2.1456\n",
      "iter 69 / 288 1085.40s 3444.97s 2.1462\n",
      "iter 70 / 288 1094.29s 3407.92s 2.1469\n",
      "iter 71 / 288 1105.01s 3377.28s 2.1452\n",
      "iter 72 / 288 1142.69s 3428.07s 2.1437\n",
      "iter 73 / 288 1160.51s 3417.95s 2.1437\n",
      "iter 74 / 288 1170.29s 3384.36s 2.1455\n",
      "iter 75 / 288 1188.36s 3374.93s 2.1443\n",
      "iter 76 / 288 1196.70s 3338.16s 2.1448\n",
      "iter 77 / 288 1205.12s 3302.33s 2.1442\n",
      "iter 78 / 288 1215.57s 3272.69s 2.1420\n",
      "iter 79 / 288 1225.76s 3242.84s 2.1431\n",
      "iter 80 / 288 1233.26s 3206.49s 2.1418\n",
      "iter 81 / 288 1253.25s 3202.75s 2.1410\n",
      "iter 82 / 288 1267.50s 3184.22s 2.1409\n",
      "iter 83 / 288 1284.18s 3171.76s 2.1404\n",
      "iter 84 / 288 1323.06s 3213.14s 2.1392\n",
      "iter 85 / 288 1331.91s 3180.91s 2.1414\n",
      "iter 86 / 288 1349.36s 3169.43s 2.1411\n",
      "iter 87 / 288 1359.62s 3141.19s 2.1420\n",
      "iter 88 / 288 1380.00s 3136.36s 2.1417\n",
      "iter 89 / 288 1391.40s 3111.12s 2.1408\n",
      "iter 90 / 288 1397.82s 3075.20s 2.1398\n",
      "iter 91 / 288 1407.14s 3046.23s 2.1412\n",
      "iter 92 / 288 1434.23s 3055.54s 2.1415\n",
      "iter 93 / 288 1445.80s 3031.52s 2.1408\n",
      "iter 94 / 288 1463.97s 3021.39s 2.1395\n",
      "iter 95 / 288 1483.46s 3013.77s 2.1398\n",
      "iter 96 / 288 1490.37s 2980.74s 2.1394\n",
      "iter 97 / 288 1506.90s 2967.19s 2.1395\n",
      "iter 98 / 288 1515.96s 2939.11s 2.1384\n",
      "iter 99 / 288 1526.95s 2915.09s 2.1388\n",
      "iter 100 / 288 1557.33s 2927.78s 2.1383\n",
      "iter 101 / 288 1565.58s 2898.65s 2.1386\n",
      "iter 102 / 288 1573.58s 2869.48s 2.1387\n",
      "iter 103 / 288 1585.97s 2848.59s 2.1390\n",
      "iter 104 / 288 1601.04s 2832.61s 2.1388\n",
      "iter 105 / 288 1639.94s 2858.18s 2.1381\n",
      "iter 106 / 288 1653.36s 2838.79s 2.1392\n",
      "iter 107 / 288 1661.49s 2810.56s 2.1393\n",
      "iter 108 / 288 1678.08s 2796.80s 2.1389\n",
      "iter 109 / 288 1690.15s 2775.57s 2.1400\n",
      "iter 110 / 288 1718.54s 2780.91s 2.1404\n",
      "iter 111 / 288 1728.88s 2756.86s 2.1409\n",
      "iter 112 / 288 1739.75s 2733.90s 2.1409\n",
      "iter 113 / 288 1750.32s 2710.67s 2.1414\n",
      "iter 114 / 288 1759.01s 2684.81s 2.1404\n",
      "iter 115 / 288 1772.65s 2666.68s 2.1404\n",
      "iter 116 / 288 1779.77s 2638.97s 2.1398\n",
      "iter 117 / 288 1790.09s 2616.28s 2.1390\n",
      "iter 118 / 288 1810.69s 2608.63s 2.1374\n",
      "iter 119 / 288 1827.07s 2594.74s 2.1388\n",
      "iter 120 / 288 1834.73s 2568.62s 2.1394\n",
      "iter 121 / 288 1842.58s 2543.07s 2.1397\n",
      "iter 122 / 288 1853.29s 2521.69s 2.1392\n",
      "iter 123 / 288 1867.84s 2505.64s 2.1391\n",
      "iter 124 / 288 1886.48s 2495.02s 2.1394\n",
      "iter 125 / 288 1900.36s 2478.07s 2.1395\n",
      "iter 126 / 288 1914.33s 2461.28s 2.1399\n",
      "iter 127 / 288 1930.36s 2447.15s 2.1397\n",
      "iter 128 / 288 1947.57s 2434.46s 2.1401\n",
      "iter 129 / 288 1958.55s 2414.02s 2.1407\n",
      "iter 130 / 288 1966.67s 2390.25s 2.1402\n",
      "iter 131 / 288 1982.77s 2376.29s 2.1399\n",
      "iter 132 / 288 1989.92s 2351.73s 2.1394\n",
      "iter 133 / 288 2012.73s 2345.66s 2.1392\n",
      "iter 134 / 288 2021.88s 2323.65s 2.1391\n",
      "iter 135 / 288 2039.86s 2311.84s 2.1382\n",
      "iter 136 / 288 2045.93s 2286.62s 2.1373\n",
      "iter 137 / 288 2057.03s 2267.24s 2.1377\n",
      "iter 138 / 288 2070.44s 2250.47s 2.1368\n",
      "iter 139 / 288 2090.52s 2240.92s 2.1370\n",
      "iter 140 / 288 2096.90s 2216.72s 2.1367\n",
      "iter 141 / 288 2107.73s 2197.42s 2.1362\n",
      "iter 142 / 288 2118.35s 2178.02s 2.1354\n",
      "iter 143 / 288 2128.28s 2158.05s 2.1362\n",
      "iter 144 / 288 2146.04s 2146.04s 2.1360\n",
      "iter 145 / 288 2156.34s 2126.60s 2.1343\n",
      "iter 146 / 288 2168.50s 2109.09s 2.1338\n",
      "iter 147 / 288 2189.94s 2100.55s 2.1322\n",
      "iter 148 / 288 2199.38s 2080.50s 2.1317\n",
      "iter 149 / 288 2214.97s 2066.31s 2.1325\n",
      "iter 150 / 288 2225.55s 2047.51s 2.1324\n",
      "iter 151 / 288 2234.32s 2027.16s 2.1320\n",
      "iter 152 / 288 2241.29s 2005.37s 2.1319\n",
      "iter 153 / 288 2248.57s 1984.03s 2.1312\n",
      "iter 154 / 288 2272.53s 1977.40s 2.1318\n",
      "iter 155 / 288 2280.39s 1956.72s 2.1316\n",
      "iter 156 / 288 2294.01s 1941.09s 2.1314\n",
      "iter 157 / 288 2303.31s 1921.87s 2.1313\n",
      "iter 158 / 288 2320.21s 1909.03s 2.1309\n",
      "iter 159 / 288 2329.12s 1889.67s 2.1298\n",
      "iter 160 / 288 2335.63s 1868.51s 2.1286\n",
      "iter 161 / 288 2342.40s 1847.73s 2.1280\n",
      "iter 162 / 288 2351.24s 1828.74s 2.1262\n",
      "iter 163 / 288 2359.83s 1809.69s 2.1267\n",
      "iter 164 / 288 2390.65s 1807.57s 2.1263\n",
      "iter 165 / 288 2404.45s 1792.41s 2.1260\n",
      "iter 166 / 288 2418.30s 1777.31s 2.1247\n",
      "iter 167 / 288 2425.84s 1757.65s 2.1243\n",
      "iter 168 / 288 2438.00s 1741.43s 2.1238\n",
      "iter 169 / 288 2448.53s 1724.11s 2.1230\n",
      "iter 170 / 288 2461.99s 1708.91s 2.1227\n",
      "iter 171 / 288 2484.70s 1700.06s 2.1219\n",
      "iter 172 / 288 2494.82s 1682.56s 2.1210\n",
      "iter 173 / 288 2501.82s 1663.06s 2.1202\n",
      "iter 174 / 288 2510.56s 1644.85s 2.1192\n",
      "iter 175 / 288 2529.46s 1633.31s 2.1176\n",
      "iter 176 / 288 2538.58s 1615.46s 2.1194\n",
      "iter 177 / 288 2564.55s 1608.28s 2.1186\n",
      "iter 178 / 288 2574.59s 1591.04s 2.1181\n",
      "iter 179 / 288 2581.48s 1571.96s 2.1175\n",
      "iter 180 / 288 2607.16s 1564.30s 2.1181\n",
      "iter 181 / 288 2616.37s 1546.69s 2.1175\n",
      "iter 182 / 288 2634.26s 1534.24s 2.1181\n",
      "iter 183 / 288 2650.25s 1520.63s 2.1182\n",
      "iter 184 / 288 2656.54s 1501.52s 2.1180\n",
      "iter 185 / 288 2666.67s 1484.69s 2.1186\n",
      "iter 186 / 288 2677.06s 1468.06s 2.1183\n",
      "iter 187 / 288 2688.82s 1452.25s 2.1188\n",
      "iter 188 / 288 2712.82s 1442.99s 2.1194\n",
      "iter 189 / 288 2726.80s 1428.32s 2.1194\n",
      "iter 190 / 288 2734.66s 1410.51s 2.1192\n",
      "iter 191 / 288 2744.83s 1393.97s 2.1185\n",
      "iter 192 / 288 2752.64s 1376.32s 2.1178\n",
      "iter 193 / 288 2769.43s 1363.19s 2.1178\n",
      "iter 194 / 288 2779.22s 1346.63s 2.1176\n",
      "iter 195 / 288 2793.24s 1332.16s 2.1170\n",
      "iter 196 / 288 2826.06s 1326.52s 2.1171\n",
      "iter 197 / 288 2834.02s 1309.12s 2.1171\n",
      "iter 198 / 288 2846.63s 1293.92s 2.1168\n",
      "iter 199 / 288 2862.13s 1280.05s 2.1171\n",
      "iter 200 / 288 2871.87s 1263.62s 2.1172\n",
      "iter 201 / 288 2895.67s 1253.35s 2.1177\n",
      "iter 202 / 288 2905.77s 1237.11s 2.1179\n",
      "iter 203 / 288 2923.08s 1223.95s 2.1174\n",
      "iter 204 / 288 2940.21s 1210.68s 2.1166\n",
      "iter 205 / 288 2947.51s 1193.38s 2.1166\n",
      "iter 206 / 288 2960.80s 1178.57s 2.1168\n",
      "iter 207 / 288 2971.41s 1162.72s 2.1167\n",
      "iter 208 / 288 2978.92s 1145.74s 2.1170\n",
      "iter 209 / 288 2991.33s 1130.70s 2.1172\n",
      "iter 210 / 288 3007.44s 1117.05s 2.1173\n",
      "iter 211 / 288 3021.54s 1102.65s 2.1168\n",
      "iter 212 / 288 3036.30s 1088.49s 2.1163\n",
      "iter 213 / 288 3059.40s 1077.25s 2.1159\n",
      "iter 214 / 288 3104.35s 1073.47s 2.1155\n",
      "iter 215 / 288 3123.50s 1060.54s 2.1159\n",
      "iter 216 / 288 3134.09s 1044.70s 2.1156\n",
      "iter 217 / 288 3145.65s 1029.22s 2.1146\n",
      "iter 218 / 288 3158.40s 1014.16s 2.1138\n",
      "iter 219 / 288 3189.34s 1004.86s 2.1137\n",
      "iter 220 / 288 3200.35s 989.20s 2.1134\n",
      "iter 221 / 288 3207.32s 972.35s 2.1133\n",
      "iter 222 / 288 3215.81s 956.05s 2.1132\n",
      "iter 223 / 288 3234.00s 942.65s 2.1133\n",
      "iter 224 / 288 3255.39s 930.11s 2.1122\n",
      "iter 225 / 288 3281.27s 918.75s 2.1123\n",
      "iter 226 / 288 3294.29s 903.74s 2.1124\n",
      "iter 227 / 288 3308.90s 889.17s 2.1125\n",
      "iter 228 / 288 3317.55s 873.04s 2.1117\n",
      "iter 229 / 288 3339.55s 860.41s 2.1109\n",
      "iter 230 / 288 3356.33s 846.38s 2.1104\n",
      "iter 231 / 288 3365.76s 830.51s 2.1103\n",
      "iter 232 / 288 3374.86s 814.62s 2.1100\n",
      "iter 233 / 288 3398.26s 802.16s 2.1096\n",
      "iter 234 / 288 3409.88s 786.90s 2.1091\n",
      "iter 235 / 288 3417.37s 770.73s 2.1085\n",
      "iter 236 / 288 3424.26s 754.50s 2.1076\n",
      "iter 237 / 288 3432.26s 738.59s 2.1068\n",
      "iter 238 / 288 3469.16s 728.82s 2.1067\n",
      "iter 239 / 288 3483.85s 714.26s 2.1070\n",
      "iter 240 / 288 3500.22s 700.04s 2.1066\n",
      "iter 241 / 288 3507.89s 684.11s 2.1067\n",
      "iter 242 / 288 3516.59s 668.44s 2.1067\n",
      "iter 243 / 288 3529.20s 653.56s 2.1064\n",
      "iter 244 / 288 3540.35s 638.42s 2.1067\n",
      "iter 245 / 288 3554.85s 623.91s 2.1064\n",
      "iter 246 / 288 3568.52s 609.26s 2.1065\n",
      "iter 247 / 288 3582.31s 594.64s 2.1058\n",
      "iter 248 / 288 3598.46s 580.40s 2.1053\n",
      "iter 249 / 288 3609.78s 565.39s 2.1049\n",
      "iter 250 / 288 3621.11s 550.41s 2.1047\n",
      "iter 251 / 288 3631.63s 535.34s 2.1047\n",
      "iter 252 / 288 3642.11s 520.30s 2.1045\n",
      "iter 253 / 288 3663.79s 506.85s 2.1048\n",
      "iter 254 / 288 3677.17s 492.22s 2.1041\n",
      "iter 255 / 288 3688.81s 477.38s 2.1046\n",
      "iter 256 / 288 3700.77s 462.60s 2.1039\n",
      "iter 257 / 288 3708.88s 447.37s 2.1029\n",
      "iter 258 / 288 3718.28s 432.36s 2.1024\n",
      "iter 259 / 288 3736.59s 418.38s 2.1026\n",
      "iter 260 / 288 3751.01s 403.96s 2.1028\n",
      "iter 261 / 288 3765.00s 389.48s 2.1025\n",
      "iter 262 / 288 3775.61s 374.68s 2.1019\n",
      "iter 263 / 288 3797.13s 360.94s 2.1010\n",
      "iter 264 / 288 3814.26s 346.75s 2.1012\n",
      "iter 265 / 288 3828.32s 332.27s 2.1015\n",
      "iter 266 / 288 3834.92s 317.17s 2.1008\n",
      "iter 267 / 288 3862.76s 303.81s 2.1009\n",
      "iter 268 / 288 3873.15s 289.04s 2.1010\n",
      "iter 269 / 288 3879.76s 274.04s 2.1012\n",
      "iter 270 / 288 3889.87s 259.32s 2.1010\n",
      "iter 271 / 288 3912.49s 245.43s 2.1013\n",
      "iter 272 / 288 3925.23s 230.90s 2.1008\n",
      "iter 273 / 288 3934.74s 216.19s 2.1008\n",
      "iter 274 / 288 3945.58s 201.60s 2.1012\n",
      "iter 275 / 288 3959.07s 187.16s 2.1009\n",
      "iter 276 / 288 3968.41s 172.54s 2.1000\n",
      "iter 277 / 288 3976.18s 157.90s 2.0996\n",
      "iter 278 / 288 3991.84s 143.59s 2.0995\n",
      "iter 279 / 288 4006.45s 129.24s 2.0990\n",
      "iter 280 / 288 4014.62s 114.70s 2.0986\n",
      "iter 281 / 288 4030.20s 100.40s 2.0985\n",
      "iter 282 / 288 4046.47s 86.10s 2.0987\n",
      "iter 283 / 288 4060.84s 71.75s 2.0987\n",
      "iter 284 / 288 4069.05s 57.31s 2.0987\n",
      "iter 285 / 288 4075.42s 42.90s 2.0980\n",
      "iter 286 / 288 4102.38s 28.69s 2.0974\n",
      "iter 287 / 288 4125.56s 14.37s 2.0980\n",
      "iter 288 / 288 4133.45s 0.00s 2.0971\n",
      "iter 289 / 288 4146.61s -14.35s 2.0966\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from time import time\n",
    "\n",
    "num_epochs = 1\n",
    "lossfct = nn.CrossEntropyLoss()\n",
    "for _ in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    start_time = time()\n",
    "    for i_step, (X_batch, lengths, y_batch) in enumerate(dataloader, 1):\n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_batch)\n",
    "        \n",
    "        # run the embeddings through the LSTM\n",
    "        packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "        output, *_ = lstm(packed)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # run the encoded last hidden state through the classifier\n",
    "        last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "        output = classifier(last_hidden_states)\n",
    "\n",
    "        loss = lossfct(output, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        delta = time() - start_time\n",
    "        avg_delta = delta / i_step\n",
    "        clip_grad_norm_(optimizer_parameters, 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        n_total = len(X_train) // dataloader.batch_size\n",
    "        n_remaining = n_total - i_step\n",
    "        print('iter', i_step, '/', n_total, f'{delta:.2f}s', f'{avg_delta*n_remaining:.2f}s', f'{train_loss / i_step:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3528, 1.5024, 1.6414, 1.2185, 0.4896, 1.8219, 1.4599, 2.0353, 1.7341],\n",
       "        [0.2771, 1.6668, 1.4966, 1.3643, 0.5157, 1.8477, 1.5806, 1.8347, 1.7740],\n",
       "        [0.3511, 1.5061, 1.6381, 1.2217, 0.4902, 1.8225, 1.4627, 2.0308, 1.7350],\n",
       "        [0.3546, 1.4984, 1.6449, 1.2149, 0.4889, 1.8212, 1.4570, 2.0402, 1.7331]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3063, 32, 300])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.permute(1, 0, 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_test = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_test.text]\n",
    "y_test = label_encoder.transform(gnad_test.category)\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_test]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_test), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_test):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_test))\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm.to(DEVICE)\n",
    "pred = []\n",
    "for i_step, (X_batch, lengths, y_batch) in enumerate(dataloader, 1):\n",
    "    X_batch = X_batch.to(DEVICE)\n",
    "    # run the word indices through the embedding layer and then the LSTM\n",
    "    embed = emb(X_batch)\n",
    "\n",
    "    # run the embeddings through the LSTM\n",
    "    packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "    output, *_ = lstm(packed)\n",
    "    output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    # run the encoded last hidden state through the classifier\n",
    "    last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "    output = classifier(last_hidden_states)\n",
    "\n",
    "    _, pred_ = F.log_softmax(output, dim=-1).max(dim=-1)\n",
    "    pred.extend(pred_.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Etat       0.00      0.00      0.00        67\n",
      "       Inland       0.00      0.00      0.00       102\n",
      "International       0.00      0.00      0.00       151\n",
      "       Kultur       0.00      0.00      0.00        54\n",
      "     Panorama       0.17      0.88      0.29       168\n",
      "        Sport       0.53      0.07      0.12       120\n",
      "          Web       0.13      0.12      0.13       168\n",
      "   Wirtschaft       0.00      0.00      0.00       141\n",
      " Wissenschaft       0.00      0.00      0.00        57\n",
      "\n",
      "     accuracy                           0.17      1028\n",
      "    macro avg       0.09      0.12      0.06      1028\n",
      " weighted avg       0.11      0.17      0.08      1028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder / Decoder architecture and Transfer Learning\n",
    "\n",
    "- Howard et al.: [_Universal Language Model Fine-tuning for Text Classification._](https://www.aclweb.org/anthology/P18-1031/) ACL (1) 2018: 328-339\n",
    "- Jeremy Howard and Sebastian Ruder [Introducing state of the art text classification with universal language models](http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) 15 May 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "lstm_stack = nn.LSTM(ft_vec.vectors[0].shape[0], 64, num_layers=3, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sport</td>\n",
       "      <td>21-Jähriger fällt wohl bis Saisonende aus. Wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>'Erfundene Bilder zu Filmen, die als verloren ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Web</td>\n",
       "      <td>Der frischgekürte CEO Sundar Pichai setzt auf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Putin: \"Einigung, dass wir Menge auf Niveau vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Estland sieht den künftigen österreichischen P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Bernd Saurer war Bridge-Juniorenweltmeister un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9241</td>\n",
       "      <td>International</td>\n",
       "      <td>Sandhere soll in vergangener Woche bei Luftang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9242</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Derzeit Konzeptgruppe in Berlin – Kein Komment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9243</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Landeshauptmann will den vierten Regierungssit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9244</td>\n",
       "      <td>Panorama</td>\n",
       "      <td>Er ist einer von Millionen syrischen Flüchtlin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9245 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0             Sport  21-Jähriger fällt wohl bis Saisonende aus. Wie...\n",
       "1            Kultur  'Erfundene Bilder zu Filmen, die als verloren ...\n",
       "2               Web  Der frischgekürte CEO Sundar Pichai setzt auf ...\n",
       "3        Wirtschaft  Putin: \"Einigung, dass wir Menge auf Niveau vo...\n",
       "4            Inland  Estland sieht den künftigen österreichischen P...\n",
       "...             ...                                                ...\n",
       "9240         Inland  Bernd Saurer war Bridge-Juniorenweltmeister un...\n",
       "9241  International  Sandhere soll in vergangener Woche bei Luftang...\n",
       "9242     Wirtschaft  Derzeit Konzeptgruppe in Berlin – Kein Komment...\n",
       "9243         Inland  Landeshauptmann will den vierten Regierungssit...\n",
       "9244       Panorama  Er ist einer von Millionen syrischen Flüchtlin...\n",
       "\n",
       "[9245 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnad_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_X_data = [word2idx[w] for sent in gnad_train.data for w in sent.split()]\n",
    "lm_y_data = lm_X_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "data = TensorDataset(X, y)\n",
    "sampler = RandomSampler(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained AWD-LSTM from fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  440M  100  440M    0     0  8935k      0  0:00:50  0:00:50 --:--:-- 9368k00:58  0:00:15  0:00:43 8796k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/fwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  440M  100  440M    0     0  6619k      0  0:01:08  0:01:08 --:--:-- 6592k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/fwd_wt103_enc.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4063k  100 4063k    0     0  2457k      0  0:00:01  0:00:01 --:--:-- 2456k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/itos_wt103.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import text\n",
    "\n",
    "# learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('itos_wt103.pkl', 'rb') as fh:\n",
    "    lm_vocab = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238462"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "gnad_train, _ = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnad_train.to_csv('lm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sport</td>\n",
       "      <td>21-Jähriger fällt wohl bis Saisonende aus. Wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>'Erfundene Bilder zu Filmen, die als verloren ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Web</td>\n",
       "      <td>Der frischgekürte CEO Sundar Pichai setzt auf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>Putin: \"Einigung, dass wir Menge auf Niveau vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Inland</td>\n",
       "      <td>Estland sieht den künftigen österreichischen P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                               text\n",
       "0       Sport  21-Jähriger fällt wohl bis Saisonende aus. Wie...\n",
       "1      Kultur  'Erfundene Bilder zu Filmen, die als verloren ...\n",
       "2         Web  Der frischgekürte CEO Sundar Pichai setzt auf ...\n",
       "3  Wirtschaft  Putin: \"Einigung, dass wir Menge auf Niveau vo...\n",
       "4      Inland  Estland sieht den künftigen österreichischen P..."
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnad_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai.datasets.untar_data(url: str, fname: Union[pathlib.Path, str] = None, dest: Union[pathlib.Path, str] = None, data=True, force_download=False) -> pathlib.Path>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "path = fastai.datasets.untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/mattilyra/.fastai/data/imdb_sample')"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = text.data.TextLMDataBunch.from_csv(path, 'texts.csv')\n",
    "awd_lstm = text.language_model_learner(data_lm, text.AWD_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck? What made this the xxunk successful triumph it was ? Was it casting , music , imagination , ingenuity , or luck ? No , no , no , no , no , no , no , no ! No , no , no , no'"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm.beam_search('What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck?',\n",
    "                     n_words=25,\n",
    "                     temperature=5.95,\n",
    "                     top_k=25,\n",
    "                     beam_sz=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
