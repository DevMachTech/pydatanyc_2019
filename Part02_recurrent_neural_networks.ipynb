{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "- Afshine Amidi and Shervine Amidi: [Recurrent Neural Networks Cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "- Chris Olah: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Chris Olah, Shan Carter: [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns)\n",
    "- Andreas Madsen: [Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns)\n",
    "- Michael Nguyen: [Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "- Michael Nguyen: [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "---\n",
    "\n",
    "- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)\n",
    "  > \"_For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL._\"\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Natural language does not usually come in neatly packaged fixed length sequences. The simple feed forward neural network from part 1, however, assumed that we can cut the documents down to 100 words. For some applications and data sources this may be approriate for others not. For instance, cutting tweets down to 100 or 200 words would not be that unreasonable, but cutting wikipedia articles would.\n",
    "\n",
    "Recurrent neural networks get around, to some extent, this limitation on the sequence length. Given infinite time they can consume an infinite sequence. The network maintains an internal state that keeps track of what is important and what isn't. There are [lots of different kinds of recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) the main difference between being the way in which the internal state is updated and maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what percentage of the documents are below our document length threshold?\n",
    "sum([True for d in gnad_train.text if len(d.split()) > 100]) / len(gnad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_articles = [idx for idx, d in enumerate(gnad_train.text) if len(d.split()) > 100]\n",
    "print(gnad_train.text[long_articles[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Idea Behind RNNs\n",
    "\n",
    "![](./img/1280px-Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "\n",
    "<sub>Image By François Deloche, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elman Network\n",
    "\n",
    "$$ h_t = \\sigma{(W_hx_t + U_hh_{t-1}+b_h)} $$\n",
    "$$ y_t = \\sigma{(W_yh_t + b_y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Elman Network in Practice\n",
    "\n",
    "PyTorch has an implementation of an Elman network as the base `nn.RNN` class. The class enables uni- or bi-directional training and supports an arbitrary number of network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "ft_vec = KeyedVectors.load_word2vec_format('./cc.de.300.vec.SMALL.gz')\n",
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.concatenate([np.random.rand(1, 300), ft_vec.vectors])\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "rnn = nn.RNN(ft_vec.vectors.shape[1], 10, num_layers=1)\n",
    "\n",
    "sentence = 'Die neue BER wird bald geöffnet'.split()\n",
    "word_idx = [word2idx[w] for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can feed individual documents through the embedding layer ...\n",
    "emb(torch.LongTensor(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and then feed the embedded document through the RNN\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load german word embeddings from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "ft_vec_DE = KeyedVectors.load_word2vec_format('./cc.de.300.vec.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec_DE.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_train = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_train.text]\n",
    "y_train = label_encoder.fit_transform(gnad_train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "UNK = np.random.rand(1, 300)\n",
    "vectors = np.concatenate([UNK, ft_vec_DE.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A strange training loop (??)\n",
    "\n",
    "Let's train an LSTM classifier that uses that last hidden state from each sequence (document) as the representation for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = False\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=1, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "h, c = None, None\n",
    "lossfct = nn.CrossEntropyLoss()\n",
    "for _ in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    start_time = time()\n",
    "    for i_step, (X_, y_) in enumerate(zip(X_train, y_train), 1):\n",
    "        X_ = torch.LongTensor(X_)\n",
    "        y_ = torch.LongTensor([y_])\n",
    "        \n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_).unsqueeze(dim=1)\n",
    "        output, *_ = lstm(embed)\n",
    "        output = classifier(output[-1, :, :])\n",
    "\n",
    "        loss = lossfct(output, y_)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        if i_step > 0 and i_step % 64 == 0:\n",
    "            delta = time() - start_time\n",
    "            avg_delta = delta // (i_step // 64)\n",
    "            clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            n_total = len(X_train) // 64\n",
    "            n_remaining = n_total - (i_step // 64)\n",
    "            print('iter', i_step // 64, '/', n_total, f'{delta:.2f}s', f'{avg_delta*n_remaining:.2f}s', f'{train_loss / i_step:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop above goes through one document at a time, this is very inefficient and unlikely to yield good performance. However, bacthing together variable length sequences requires a little bit of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_train]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_train), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_train):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_train))\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "X_tmp, len_tmp, _ = dataset[:4]\n",
    "embed = emb(X_tmp)\n",
    "packed = pack_padded_sequence(embed, len_tmp, enforce_sorted=False, batch_first=True)\n",
    "output, *_ = lstm(packed)\n",
    "X, _ = pad_packed_sequence(output, batch_first=True)\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = True\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=4, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer_parameters = list(emb.parameters()) + list(lstm.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.Adam(optimizer_parameters, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_epochs = 100\n",
    "lossfct = nn.CrossEntropyLoss().to(DEVICE)\n",
    "start_time = time()\n",
    "for i_epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    lstm.train()\n",
    "    classifier.train()\n",
    "    emb.to(DEVICE)\n",
    "    lstm.to(DEVICE)\n",
    "    classifier.to(DEVICE)\n",
    "    for i_step, batch in enumerate(dataloader, 1):\n",
    "        X_batch, lengths, y_batch = (b.to(DEVICE) for b in batch)\n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_batch)\n",
    "        \n",
    "        # run the embeddings through the LSTM\n",
    "        packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "        output, *_ = lstm(packed)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # run the encoded last hidden state through the classifier\n",
    "        last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "        output = classifier(last_hidden_states)\n",
    "\n",
    "        loss = lossfct(output, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    delta = time() - start_time\n",
    "    avg_delta = delta / (i_epoch + 1)\n",
    "    remaining = avg_delta * (num_epochs - i_epoch)\n",
    "    print('epoch', i_epoch, '/', num_epochs, f'{train_loss / i_step:.4f}',\n",
    "          f'avg epoch {datetime.fromtimestamp(avg_delta):%M:%S}',\n",
    "          f'total elapsed {datetime.fromtimestamp(delta):%M:%S}',\n",
    "          f'remaining {datetime.fromtimestamp(remaining):%M:%S}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_test = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_test.text]\n",
    "y_test = label_encoder.transform(gnad_test.category)\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_test]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_test), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_test):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "test_dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_test))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm.to(DEVICE)\n",
    "pred = []\n",
    "for i_step, (X_batch, lengths, y_batch) in enumerate(test_dataloader, 1):\n",
    "    X_batch = X_batch.to(DEVICE)\n",
    "    # run the word indices through the embedding layer and then the LSTM\n",
    "    embed = emb(X_batch)\n",
    "\n",
    "    # run the embeddings through the LSTM\n",
    "    packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "    output, *_ = lstm(packed)\n",
    "    output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    # run the encoded last hidden state through the classifier\n",
    "    last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "    output = classifier(last_hidden_states)\n",
    "\n",
    "    _, pred_ = F.log_softmax(output, dim=-1).max(dim=-1)\n",
    "    pred.extend(pred_.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (64, 1-layer) + Linear\n",
    "    100 epochs\n",
    "    SGD lr=0.1 / momentum = 0.9\n",
    "    ~ 25 minutes\n",
    "    ** -----------------------------------------------------\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.80      0.70      0.75        67\n",
    "           Inland       0.66      0.72      0.69       102\n",
    "    International       0.87      0.81      0.84       151\n",
    "           Kultur       0.73      0.74      0.73        54\n",
    "         Panorama       0.78      0.71      0.75       168\n",
    "            Sport       0.95      0.98      0.97       120\n",
    "              Web       0.90      0.89      0.89       168\n",
    "       Wirtschaft       0.81      0.70      0.75       141\n",
    "     Wissenschaft       0.53      0.89      0.66        57\n",
    "\n",
    "         accuracy                           0.80      1028\n",
    "        macro avg       0.78      0.79      0.78      1028\n",
    "     weighted avg       0.81      0.80      0.80      1028\n",
    "     \n",
    "     \n",
    "     \n",
    "\n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (64, 1-layer) + Linear\n",
    "    100 epochs\n",
    "    Adam lr=0.01\n",
    "    ~ 25 minutes\n",
    "    ** -----------------------------------------------------\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.81      0.75      0.78        67\n",
    "           Inland       0.71      0.74      0.72       102\n",
    "    International       0.86      0.83      0.84       151\n",
    "           Kultur       0.85      0.81      0.83        54\n",
    "         Panorama       0.74      0.75      0.74       168\n",
    "            Sport       0.97      0.96      0.97       120\n",
    "              Web       0.89      0.89      0.89       168\n",
    "       Wirtschaft       0.77      0.79      0.78       141\n",
    "     Wissenschaft       0.81      0.88      0.84        57\n",
    "\n",
    "         accuracy                           0.82      1028\n",
    "        macro avg       0.82      0.82      0.82      1028\n",
    "     weighted avg       0.82      0.82      0.82      1028\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (128, 2-layer, bidir) + Linear\n",
    "    fine tuned embeddings\n",
    "    100 epochs\n",
    "    Adam lr=0.01\n",
    "    ~ 55 minutes\n",
    "                  -----------------------------------------------------\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.86      0.76      0.81        67\n",
    "           Inland       0.73      0.78      0.76       102\n",
    "    International       0.88      0.86      0.87       151\n",
    "           Kultur       0.85      0.83      0.84        54\n",
    "         Panorama       0.79      0.76      0.77       168\n",
    "            Sport       0.98      0.98      0.98       120\n",
    "              Web       0.90      0.91      0.91       168\n",
    "       Wirtschaft       0.81      0.80      0.81       141\n",
    "     Wissenschaft       0.82      0.96      0.89        57\n",
    "\n",
    "         accuracy                           0.85      1028\n",
    "        macro avg       0.85      0.85      0.85      1028\n",
    "     weighted avg       0.85      0.85      0.85      1028\n",
    "     \n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (64, 4-layer, bidir) + Linear\n",
    "    fine tuned embeddings\n",
    "    100 epochs\n",
    "    Adam lr=0.01\n",
    "    ~ 70 minutes\n",
    "                  -----------------------------------------------------\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.87      0.69      0.77        67\n",
    "           Inland       0.75      0.75      0.75       102\n",
    "    International       0.83      0.85      0.84       151\n",
    "           Kultur       0.81      0.78      0.79        54\n",
    "         Panorama       0.73      0.82      0.78       168\n",
    "            Sport       0.99      0.98      0.99       120\n",
    "              Web       0.92      0.93      0.92       168\n",
    "       Wirtschaft       0.84      0.79      0.82       141\n",
    "     Wissenschaft       0.91      0.86      0.88        57\n",
    "\n",
    "         accuracy                           0.84      1028\n",
    "        macro avg       0.85      0.83      0.84      1028\n",
    "     weighted avg       0.85      0.84      0.84      1028\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning, the Encoder Decoder architecture and Attention\n",
    "\n",
    "The document classification example above used the last hidden state of the LSTM cell(s) as _input_ to a linear classifier layer. You can think of the LSTM stack as an encoder that takes the input sequence and creates some fixed length, real values dense representation of it. This encoding, or shall we say embedding, is then used by the classifier (the _decoder_), to create the final predictions.\n",
    "\n",
    "The usage of \"_encoder_\" and \"_decoder_\" is a slight abuse of terminology, normally these terms are used in the context of the machine translation where an encoder RNN consumes an input sequence, outputs an encoded state that is then used by the decoder RNN to produce an output sequence of possibly different length. The original `seq2seq` models were improved by [Bahdanau (ICLR 2015)](https://arxiv.org/abs/1409.0473) introducing an attention mechanism.\n",
    "\n",
    "Instead of the encoder RNN producing just a single final encoded state of the input sequence, the encoder produces a series of encoded states, one for each item in the input. The decoder uses attention weights to select which parts of the input to pay attention to when producing each element in the output sequence.\n",
    "\n",
    "I won't cover attention in any more detail here, but there is a Notebook tutorial in the pytorch documentation for neural machine translation with attention.\n",
    "\n",
    "- Bahdanau et al.: [_Neural Machine Translation by Jointly Learning to Align and Translate._](https://arxiv.org/abs/1409.0473) ICLR 2015\n",
    "- Lilian Weng: [Attention, Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "- [NLP from scratch: translation with a sequence to sequence network and attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings and Transfer Learning\n",
    "\n",
    "Word meaning is sensitive to context, but the pretrained word embeddings from fastText we've been using thus far are not.\n",
    "\n",
    "Let's say we are building a system that needs to find origin and destination mentions from text. Clearly origins and destinations are all locations, but whether some location is an origin, a destination or neither is context dependant.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    I live in Berlin and usually cycle to work.\n",
    "              [LOC]\n",
    "              [ - ]\n",
    "    \n",
    "    \n",
    "    Can you book me a flight from Berlin to New York\n",
    "                                  [LOC]\n",
    "                                  [org]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The encoder / decoder architecture serves as a good example for how contextual word embeddings work. Similar to how the encoder RNN can be used to encode a source document, it can also be used to encode single words or parts of words _in context_.\n",
    "\n",
    "- Peters et al.: [_Deep contextualized word representations._](https://www.aclweb.org/anthology/N18-1202/) NAACL-HLT 2018: 2227-2237\n",
    "- Akbik et al.: [_Pooled Contextualized Embeddings for Named Entity Recognition._](https://www.aclweb.org/anthology/N19-1078/) NAACL-HLT (1) 2019: 724-728\n",
    "- Howard et al.: [_Universal Language Model Fine-tuning for Text Classification._](https://www.aclweb.org/anthology/P18-1031/) ACL (1) 2018: 328-339\n",
    "- Jeremy Howard and Sebastian Ruder [Introducing state of the art text classification with universal language models](http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) 15 May 2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained AWD-LSTM from fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/fwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/fwd_wt103_enc.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O http://files.fast.ai/models/wt103/itos_wt103.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "path = fastai.datasets.untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = text.data.TextLMDataBunch.from_csv(path, 'texts.csv')\n",
    "awd_lstm = text.language_model_learner(data_lm, text.AWD_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm.beam_search('What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck?',\n",
    "                     n_words=25,\n",
    "                     temperature=0.95,\n",
    "                     top_k=25,\n",
    "                     beam_sz=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
