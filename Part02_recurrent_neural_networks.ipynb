{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.3.0', True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "- Chris Olah [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Chris Olah, Shan Carter [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns)\n",
    "- Andreas Madsen [Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns)\n",
    "- Chris Nicholson [A Beginner's Guide to LSTMs and Recurrent Neural Networks](https://skymind.ai/wiki/lstm)\n",
    "- Michael Nguyen [Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "- Michael Nguyen [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "---\n",
    "\n",
    "- [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)\n",
    "  > \"_For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL._\"\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Natural language does not usually come in neatly packaged fixed length sequences. The simple feed forward neural network from part 1, however, assumed that we can cut the documents down to 100 words. For some applications and data sources this may be approriate for others not. For instance, cutting tweets down to 100 or 200 words would not be that unreasonable, but cutting wikipedia articles would.\n",
    "\n",
    "Recurrent neural networks get around, to some extent, this limitation on the sequence length. Given infinite time they can consume an infinite sequence. The network maintains an internal state that keeps track of what is important and what isn't. There are [lots of different kinds of recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) the main difference between being the way in which the internal state is updated and maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415900486749594"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage of the 20 newsgroups documents are below our document length threshold?\n",
    "sum([True for d in gnad_train.text if len(d.split()) > 100]) / len(gnad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Österreich bleibt in der EM-Qualifikation für 2016 ungeschlagen, ja, feiert in und gegen Russland den sechsten Auswärtssieg en suite. Marc Janko macht mit einem Traum von einem Tor die Reise nach Frankreich beinahe zur Gewissheit. Moskau - Sie wollten unmittelbar vor dem Urlaub die allerletzten Kräfte mobilisieren, im abschließenden Saisonspiel einen Kampf bis zum Umfallen abliefern, den Russen zeigen, wer Tabellenführer der Gruppe G ist. Und die österreichische Fußballnationalmannschaft mobilisierte, zeigte. Umgefallen ist sie nicht. Teamchef Marcel Koller konnte, vom verletzten David Alaba abgesehen (wurde durch Stefan Ilsanker ersetzt), aus dem Vollen schöpfen. Martin Harnik ist fit geworden, also lief die Einsergarnitur ein. Der Boss hatte den Auftrag erteilt, hinten kompakt zu stehen, aber in erster Linie Fußball zu spielen. Und sie spielten von Anpfiff an. Russland kam gar nicht zum Schauen, fand keine Zeit, eine Ordnung zu finden. Die Gäste pressten, drückten, kombinierten. Sie waren geistig voll auf der Höhe und konzentriert, kombinierten sich durch die nicht ausverkaufte Otkrytije Arena. Die  linke Seite mit Kapitän Christian Fuchs und Marko Arnautovic wirbelte in der Moskauer Schwüle, schon bald  war klar, dass Zlatko Junuzovic der absolute Wahnsinn ist. Er schaltete seinen Motor ein, das gesamte Feld wurde sein Arbeitsplatz, der Bremen-Legionär war Passgeber, Antreiber, Schaltzentrale, Herz, Lunge, Hirn. Jankos Bewerbung In der vierten Minute wurde Russlands Torhüter Igor Akinfejew noch von einem eigenen Verteidiger geprüft, danach sorgten die Österreicher selbst für Gefahr. 7. Minute: Arnautovic misslingt der allerletzte Pass ganz knapp. Zehnte Minute: Outeinwurf von Fuchs in die Tiefe, Junuzovic, wer sonst, scheitert. Russland wirkte gehemmt, überrascht, perplex. Trainer Fabio Capello stand wie versteinert an der Seitenlinie. Nur eine gute Aktion hat er von den Seinen schon gesehen, Robert Almer parierte einen Schuss von Oleg Schatow (28.). Aus Capellos Sicht war das Gegentor allerdings nur eine Frage der Zeit, in der 33. Minute ist es gefallen. Junuzovic, von Harnik bedient, scheitert binnen Sekunden zweimal an Akinfejew, die Verteidigung bringt den Ball nicht aus dem Gefahrenbereich. Gefahr wird im österreichischen Spitzenfußball bekanntlich durch Marc Janko personifiziert. Der 31-Jährige hebt ab, legt sich quer, mit dem Rücken zum Tor. Es war der Beginn eines großartigen Fallrückziehers, der im 1:0 endete. Adjektive wie fulminant, famos, traumhaft für sein 21. Länderspieltor (46 Einsätze) sind schamlose Untertreibungen. Jubel. Das war eines der schöneren und wichtigeren Tore meine Karriere, ich hoffe, es war nicht das letzte, sollte Janko danach sagen. Der Mann sucht übrigens einen Verein. Russland im Schock, mit dieser Sanktion war nicht zu rechnen. 35. Minute: Junuzovic spielt Julian Baumgartlinger frei, der läuft ohne Verfolger gen Tor, schießt daneben. Das 2:0 wäre die Entscheidung gewesen. Es muss schon sehr blöd hergehen Nach der Pause änderte sich das Bild doch ein bisserl drastisch. Die Russen wollten nicht als Versager dastehen, legten zu. Almer war urplötzlich nicht mehr unterbeschäftigt, die Innenverteidiger Aleksandar Dragovic und Martin Hinteregger wurden mit einer Art Walze konfrontiert. Die Österreicher hatten Teilchen ihrer Aggressivität verloren. Der erschöpfte Harnik wurde durch Marcel Sabitzer ersetzt (65.), Torschütze Janko machte Platz für Rubin Okotie (75.). Der knappe Vorsprung wurde dank einer fast schon kitschigen Aufopferung  über die Runden gebracht. Da haben die Jungs alles rausgehaut, sagte Coach Marcel Koller. Die Teilnahme an der EM-Endrunde 2016 in Frankreich ist kaum noch zu verhindern. ÖFB-Präsident Leo Windtner sprach:_Wir haben den TGV Richtung Frankreich in Bewegung gesetzt. Russland, zuvor 21 Heimspiele en suite unbesiegt, wird eher zuschauen müssen. Nochmals Janko: Jetzt muss es schon sehr blöd hergehen, dass wir das aus der Hand geben. Österreichs Lustreise zum Wiedersehen mit Frankreich nach der WM-Teilnahme 1998 wird am 5. September in Wien gegen die Republik Moldau fortgesetzt, drei Tage später werden in Stockholm die Schweden beehrt. Aber jetzt ist Urlaub. Wohlverdient. (Christian Hackl aus Moskau, 14.6.2015) Fußball-EM-Qualifikation, Gruppe G/6. Runde: Russland - Österreich 0:1 (0:1). Moskau, Otkrytije Arena, 35.000, SR Milorad Mazic (SRB) Tor: 0:1 (33.) Janko Russland: Akinfejew - Smolnikow, Nowoselzew, Beresuzki (12. Tschernow), Kombarow (72. Kerschakow) - Gluschakow, Iwanow (46. Mirantschuk) - Schatow, Schirokow, Schirkow - Kokorin Österreich: Almer - Klein, Dragovic, Hinteregger, Fuchs - Baumgartlinger, Ilsanker - Harnik (65. Sabitzer), Junuzovic (87. Prödl), Arnautovic - Janko (76. Okotie) Gelbe Karten: Kokorin bzw. Klein\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_articles = [idx for idx, d in enumerate(gnad_train.text) if len(d.split()) > 100]\n",
    "print(gnad_train.text[long_articles[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Idea Behind RNNs\n",
    "\n",
    "![](./img/1280px-Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "\n",
    "<sub>Image By François Deloche, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elman Network\n",
    "\n",
    "$$ h_t = \\sigma{(W_hx_t + U_hh_{t-1}+b_h)} $$\n",
    "$$ y_t = \\sigma{(W_yh_t + b_y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Elman Network in Practice\n",
    "\n",
    "PyTorch has an implementation of an Elman network as the base `nn.RNN` class. The class enables uni- or bi-directional training and supports an arbitrary number of network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "ft_vec = KeyedVectors.load_word2vec_format('./cc.de.300.vec.SMALL.gz')\n",
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.concatenate([np.random.rand(1, 300), ft_vec.vectors])\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "rnn = nn.RNN(ft_vec.vectors.shape[1], 10, num_layers=1)\n",
    "\n",
    "UNK = np.random.rand(1, ft_vec.vectors[0].shape[0])\n",
    "vectors = np.concatenate([UNK, ft_vec.vectors], axis=0)\n",
    "sent = 'Die neue BER wird bald geöffnet'.split()\n",
    "word_idx = [word2idx[w] for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175121"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[88478, 91678, 142125, 84236, 251, 38088]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0138, -0.0134, -0.0165,  ..., -0.0256,  0.0201, -0.0439],\n",
       "        [-0.1180, -0.0394, -0.0214,  ...,  0.0711,  0.0084, -0.0139],\n",
       "        [ 0.0351, -0.0030,  0.0232,  ...,  0.0440, -0.0155, -0.0054],\n",
       "        [-0.0824,  0.1310, -0.1151,  ...,  0.0324,  0.0836,  0.0936],\n",
       "        [-0.0111,  0.0363, -0.0078,  ..., -0.0070,  0.0040, -0.0265],\n",
       "        [ 0.0368, -0.0124,  0.0299,  ...,  0.0051,  0.0385, -0.0078]])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can feed individual documents through the embedding layer ...\n",
    "emb(torch.LongTensor(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and then feed the embedded document through the RNN\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1940, -0.0908, -0.1706,  0.3068,  0.4633, -0.4665,  0.5681,\n",
       "           0.4381,  0.3606,  0.0855],\n",
       "         [-0.3440,  0.0154, -0.2777,  0.1304,  0.3534, -0.5114,  0.4618,\n",
       "           0.5423,  0.4515,  0.1751],\n",
       "         [-0.1124,  0.0558, -0.1972,  0.3448,  0.3629, -0.5432,  0.5532,\n",
       "           0.5761,  0.4233,  0.0113],\n",
       "         [-0.2124,  0.2233,  0.1864,  0.4236,  0.0920, -0.4740,  0.2988,\n",
       "           0.5289,  0.1059,  0.1892],\n",
       "         [-0.3647,  0.4744, -0.2364,  0.4440,  0.5096, -0.4413,  0.5552,\n",
       "           0.5197,  0.3503, -0.1133],\n",
       "         [-0.1695,  0.0051, -0.1974,  0.2503,  0.4625, -0.4975,  0.5649,\n",
       "           0.6053,  0.3775,  0.0334]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0699,  0.4773, -0.2269,  0.0485,  0.3940, -0.6445, -0.1033,\n",
       "          -0.4411, -0.2875, -0.1820],\n",
       "         [ 0.4298,  0.4310,  0.2294,  0.3335, -0.0151, -0.5991, -0.5415,\n",
       "          -0.2417, -0.4159, -0.5094],\n",
       "         [-0.1932,  0.1373, -0.6091,  0.3863,  0.1393, -0.2915, -0.2880,\n",
       "          -0.3617, -0.8717, -0.1275],\n",
       "         [ 0.5143,  0.7247,  0.0362,  0.5913, -0.2482, -0.6949,  0.3882,\n",
       "          -0.2663,  0.0628,  0.2348],\n",
       "         [-0.0294,  0.0504, -0.2885, -0.1780,  0.2908, -0.3357, -0.2692,\n",
       "          -0.0353,  0.0394, -0.2128],\n",
       "         [ 0.3471,  0.2983, -0.0493,  0.2738,  0.3065, -0.1331, -0.0619,\n",
       "          -0.6883, -0.6351, -0.3907],\n",
       "         [-0.2459,  0.0986, -0.3187, -0.2063,  0.3427,  0.1184, -0.2180,\n",
       "          -0.1361, -0.6476, -0.2689]],\n",
       "\n",
       "        [[-0.0255, -0.2716, -0.1416,  0.3043,  0.2569, -0.1661, -0.1401,\n",
       "          -0.3326,  0.1700, -0.0462],\n",
       "         [-0.3487, -0.4301,  0.0090,  0.1152,  0.2997, -0.0407, -0.2095,\n",
       "          -0.6039,  0.1502,  0.0897],\n",
       "         [-0.2930, -0.4659, -0.0934,  0.3153,  0.0370, -0.0437, -0.4436,\n",
       "          -0.1195,  0.3053,  0.1824],\n",
       "         [ 0.1636, -0.4788, -0.2220,  0.2902,  0.2477, -0.3367, -0.2781,\n",
       "          -0.3178,  0.3171, -0.1148],\n",
       "         [-0.0530, -0.2067, -0.3568,  0.1635,  0.0507, -0.0873, -0.1997,\n",
       "          -0.2115,  0.3918, -0.3242],\n",
       "         [-0.1261, -0.4157,  0.2901,  0.3289,  0.2442, -0.1014, -0.3587,\n",
       "          -0.4723,  0.2314,  0.2114],\n",
       "         [-0.1883, -0.2007, -0.0660,  0.1878, -0.0409,  0.0802, -0.2443,\n",
       "          -0.0300,  0.2753, -0.0333]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Actually,'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c9f36311606a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-c9f36311606a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlong_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Actually,'"
     ]
    }
   ],
   "source": [
    "UNK = torch.FloatTensor().random_()\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(ft_vec.vectors))\n",
    "sent = news_train.data[long_articles[12]].split()\n",
    "word_idx = [word2idx[w] for w in sent]\n",
    "output, hn = rnn(emb(torch.LongTensor([word_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load german word embeddings from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "ft_vec_DE = KeyedVectors.load_word2vec_format('./cc.de.300.vec.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict((w, idx+1) for (idx, w) in enumerate(ft_vec_DE.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "gnad_train, gnad_test = utils.load_gnad()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_train = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_train.text]\n",
    "y_train = label_encoder.fit_transform(gnad_train.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "UNK = np.random.rand(1, 300)\n",
    "vectors = np.concatenate([UNK, ft_vec_DE.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A strange training loop (??)\n",
    "\n",
    "Let's train an LSTM classifier that uses that last hidden state from each sequence (document) as the representation for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattilyra/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.01 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = False\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=1, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 / 144 7.87s 1001.00s 2.2111\n",
      "iter 2 / 144 15.06s 994.00s 2.2263\n",
      "iter 3 / 144 21.86s 987.00s 2.2287\n",
      "iter 4 / 144 27.39s 840.00s 2.2228\n",
      "iter 5 / 144 33.10s 834.00s 2.2213\n",
      "iter 6 / 144 39.65s 828.00s 2.2114\n",
      "iter 7 / 144 45.28s 822.00s 2.2038\n",
      "iter 8 / 144 52.30s 816.00s 2.2002\n",
      "iter 9 / 144 60.54s 810.00s 2.1944\n",
      "iter 10 / 144 66.95s 804.00s 2.1845\n",
      "iter 11 / 144 73.32s 798.00s 2.1809\n",
      "iter 12 / 144 80.35s 792.00s 2.1778\n",
      "iter 13 / 144 87.38s 786.00s 2.1684\n",
      "iter 14 / 144 94.31s 780.00s 2.1664\n",
      "iter 15 / 144 101.28s 774.00s 2.1591\n",
      "iter 16 / 144 108.15s 768.00s 2.1529\n",
      "iter 17 / 144 115.09s 762.00s 2.1449\n",
      "iter 18 / 144 121.91s 756.00s 2.1424\n",
      "iter 19 / 144 129.18s 750.00s 2.1438\n",
      "iter 20 / 144 135.88s 744.00s 2.1431\n",
      "iter 21 / 144 142.04s 738.00s 2.1436\n",
      "iter 22 / 144 148.78s 732.00s 2.1435\n",
      "iter 23 / 144 155.89s 726.00s 2.1443\n",
      "iter 24 / 144 161.91s 720.00s 2.1411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-a19656122a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pydatanyc_2019-b2AkOBOU/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "h, c = None, None\n",
    "lossfct = nn.CrossEntropyLoss()\n",
    "for _ in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    start_time = time()\n",
    "    for i_step, (X_, y_) in enumerate(zip(X_train, y_train), 1):\n",
    "        X_ = torch.LongTensor(X_)\n",
    "        y_ = torch.LongTensor([y_])\n",
    "        \n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_).unsqueeze(dim=1)\n",
    "        output, *_ = lstm(embed)\n",
    "        output = classifier(output[-1, :, :])\n",
    "        loss = lossfct(output, y_)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        if i_step > 0 and i_step % 64 == 0:\n",
    "            delta = time() - start_time\n",
    "            avg_delta = delta // (i_step // 64)\n",
    "            clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            n_total = len(X_train) // 64\n",
    "            n_remaining = n_total - (i_step // 64)\n",
    "            print('iter', i_step // 64, '/', n_total, f'{delta:.2f}s', f'{avg_delta*n_remaining:.2f}s', f'{train_loss / i_step:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    What makes the training loop strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "- iterate through the dataset one document at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Iterating through a data set one item at a time is very inefficient and unlikely to yield good performance (_Stochastic_ GD). However, bacthing together variable length sequences requires a little bit of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create batches, or mini batches, from variable sized \"_tensors_\" we need to first make all tensors the same length either by padding short documents or truncating long documents or both.\n",
    "\n",
    "Then, in the training loop we remove the padding before the instances are passed to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# record document lenghts\n",
    "doc_lengths = [len(doc) for doc in X_train]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "# pad all documents to the length of the longest document\n",
    "# notice that this is very inefficient from a memory perspective\n",
    "data = np.zeros((len(X_train), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_train):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_train))\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3063])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[132813, 130692,  82087,  ...,      0,      0,      0],\n",
       "         [     0,  85372, 104301,  ...,      0,      0,      0],\n",
       "         [ 28850, 148805,  88260,  ...,      0,      0,      0],\n",
       "         [     0,      0, 143159,  ...,      0,      0,      0],\n",
       "         [104810,  27421, 144802,  ...,      0,      0,      0]]),\n",
       " tensor([ 63, 578, 277,  97, 619]),\n",
       " tensor([5, 3, 6, 7, 1]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 578, 128])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "X_tmp, len_tmp, _ = dataset[:4]\n",
    "embed = emb(X_tmp)\n",
    "\n",
    "packed = pack_padded_sequence(embed, len_tmp, enforce_sorted=False, batch_first=True)\n",
    "output, *_ = lstm(packed)\n",
    "X, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hmmm ...\n",
    "\n",
    "That's a little strange, we passed in somethig that has `3063` columns and got back something with only 578. Why?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM implementation in `pytorch` takes the packed padded sequence of elements and applies the LSTM to _only_ the non padded parts of each element in the sequence.\n",
    "\n",
    "How do we find the _last hidden state_ of each document in the batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 3, 4, 5, 0, 0, 0, 0],\n",
       "       [1, 2, 3, 4, 5, 6, 0, 0, 0],\n",
       "       [1, 2, 3, 4, 5, 6, 7, 0, 0],\n",
       "       [1, 2, 3, 4, 5, 6, 7, 8, 0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.asarray([list(range(1, n)) + [0] * (10 - n) for n in [5, 6, 7, 8, 9]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths = np.asarray([4, 5, 6, 7, 8])\n",
    "X[range(0, 5), doc_lengths - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "bidirectional = True\n",
    "emb = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "lstm = nn.LSTM(vectors.shape[1], 64, num_layers=4, bidirectional=bidirectional, dropout=0.01)\n",
    "classifier = nn.Linear(lstm.hidden_size if not bidirectional else lstm.hidden_size * 2, len(label_encoder.classes_))\n",
    "optimizer_parameters = list(emb.parameters()) + list(lstm.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.Adam(optimizer_parameters, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 / 100 2.0936 avg epoch 00:43 total elapsed 00:43 remaining 12:42\n",
      "epoch 1 / 100 2.1721 avg epoch 00:43 total elapsed 01:26 remaining 11:39\n",
      "epoch 2 / 100 2.1082 avg epoch 00:43 total elapsed 02:10 remaining 10:49\n",
      "epoch 3 / 100 2.0824 avg epoch 00:43 total elapsed 02:53 remaining 10:02\n",
      "epoch 4 / 100 1.9883 avg epoch 00:43 total elapsed 03:36 remaining 09:17\n",
      "epoch 5 / 100 1.7496 avg epoch 00:43 total elapsed 04:19 remaining 08:31\n",
      "epoch 6 / 100 1.5657 avg epoch 00:43 total elapsed 05:02 remaining 07:46\n",
      "epoch 7 / 100 1.3766 avg epoch 00:43 total elapsed 05:46 remaining 07:03\n",
      "epoch 8 / 100 1.2049 avg epoch 00:43 total elapsed 06:29 remaining 06:19\n",
      "epoch 9 / 100 1.0891 avg epoch 00:43 total elapsed 07:12 remaining 05:36\n",
      "epoch 10 / 100 1.0291 avg epoch 00:43 total elapsed 07:55 remaining 04:52\n",
      "epoch 11 / 100 0.9333 avg epoch 00:43 total elapsed 08:39 remaining 04:09\n",
      "epoch 12 / 100 0.7869 avg epoch 00:43 total elapsed 09:22 remaining 03:25\n",
      "epoch 13 / 100 0.6904 avg epoch 00:43 total elapsed 10:05 remaining 02:42\n",
      "epoch 14 / 100 0.6063 avg epoch 00:43 total elapsed 10:48 remaining 01:59\n",
      "epoch 15 / 100 0.5513 avg epoch 00:43 total elapsed 11:32 remaining 01:16\n",
      "epoch 16 / 100 0.5282 avg epoch 00:43 total elapsed 12:15 remaining 00:32\n",
      "epoch 17 / 100 0.4934 avg epoch 00:43 total elapsed 12:58 remaining 59:49\n",
      "epoch 18 / 100 0.4149 avg epoch 00:43 total elapsed 13:41 remaining 59:05\n",
      "epoch 19 / 100 0.3654 avg epoch 00:43 total elapsed 14:24 remaining 58:22\n",
      "epoch 20 / 100 0.3241 avg epoch 00:43 total elapsed 15:07 remaining 57:38\n",
      "epoch 21 / 100 0.3553 avg epoch 00:43 total elapsed 15:51 remaining 56:55\n",
      "epoch 22 / 100 0.3093 avg epoch 00:43 total elapsed 16:34 remaining 56:11\n",
      "epoch 23 / 100 0.3071 avg epoch 00:43 total elapsed 17:17 remaining 55:27\n",
      "epoch 24 / 100 0.2788 avg epoch 00:43 total elapsed 18:00 remaining 54:44\n",
      "epoch 25 / 100 0.2484 avg epoch 00:43 total elapsed 18:43 remaining 54:00\n",
      "epoch 26 / 100 0.2271 avg epoch 00:43 total elapsed 19:26 remaining 53:17\n",
      "epoch 27 / 100 0.2198 avg epoch 00:43 total elapsed 20:09 remaining 52:33\n",
      "epoch 28 / 100 0.1873 avg epoch 00:43 total elapsed 20:52 remaining 51:50\n",
      "epoch 29 / 100 0.1721 avg epoch 00:43 total elapsed 21:36 remaining 51:07\n",
      "epoch 30 / 100 0.1700 avg epoch 00:43 total elapsed 22:19 remaining 50:23\n",
      "epoch 31 / 100 0.1888 avg epoch 00:43 total elapsed 23:02 remaining 49:40\n",
      "epoch 32 / 100 0.1518 avg epoch 00:43 total elapsed 23:45 remaining 48:57\n",
      "epoch 33 / 100 0.1823 avg epoch 00:43 total elapsed 24:28 remaining 48:14\n",
      "epoch 34 / 100 0.1913 avg epoch 00:43 total elapsed 25:11 remaining 47:30\n",
      "epoch 35 / 100 0.1307 avg epoch 00:43 total elapsed 25:55 remaining 46:47\n",
      "epoch 36 / 100 0.1385 avg epoch 00:43 total elapsed 26:38 remaining 46:04\n",
      "epoch 37 / 100 0.1031 avg epoch 00:43 total elapsed 27:21 remaining 45:21\n",
      "epoch 38 / 100 0.0908 avg epoch 00:43 total elapsed 28:04 remaining 44:38\n",
      "epoch 39 / 100 0.0822 avg epoch 00:43 total elapsed 28:48 remaining 43:55\n",
      "epoch 40 / 100 0.0866 avg epoch 00:43 total elapsed 29:31 remaining 43:12\n",
      "epoch 41 / 100 0.0965 avg epoch 00:43 total elapsed 30:14 remaining 42:29\n",
      "epoch 42 / 100 0.1153 avg epoch 00:43 total elapsed 30:57 remaining 41:45\n",
      "epoch 43 / 100 0.0871 avg epoch 00:43 total elapsed 31:41 remaining 41:02\n",
      "epoch 44 / 100 0.0884 avg epoch 00:43 total elapsed 32:24 remaining 40:19\n",
      "epoch 45 / 100 0.0825 avg epoch 00:43 total elapsed 33:07 remaining 39:36\n",
      "epoch 46 / 100 0.1018 avg epoch 00:43 total elapsed 33:50 remaining 38:53\n",
      "epoch 47 / 100 0.0579 avg epoch 00:43 total elapsed 34:33 remaining 38:09\n",
      "epoch 48 / 100 0.0312 avg epoch 00:43 total elapsed 35:17 remaining 37:26\n",
      "epoch 49 / 100 0.0226 avg epoch 00:43 total elapsed 36:00 remaining 36:43\n",
      "epoch 50 / 100 0.0179 avg epoch 00:43 total elapsed 36:43 remaining 36:00\n",
      "epoch 51 / 100 0.0164 avg epoch 00:43 total elapsed 37:26 remaining 35:17\n",
      "epoch 52 / 100 0.0180 avg epoch 00:43 total elapsed 38:09 remaining 34:33\n",
      "epoch 53 / 100 0.0232 avg epoch 00:43 total elapsed 38:52 remaining 33:50\n",
      "epoch 54 / 100 0.0197 avg epoch 00:43 total elapsed 39:36 remaining 33:07\n",
      "epoch 55 / 100 0.0242 avg epoch 00:43 total elapsed 40:19 remaining 32:23\n",
      "epoch 56 / 100 0.0166 avg epoch 00:43 total elapsed 41:02 remaining 31:40\n",
      "epoch 57 / 100 0.0146 avg epoch 00:43 total elapsed 41:45 remaining 30:57\n",
      "epoch 58 / 100 0.0282 avg epoch 00:43 total elapsed 42:28 remaining 30:14\n",
      "epoch 59 / 100 0.0463 avg epoch 00:43 total elapsed 43:11 remaining 29:30\n",
      "epoch 60 / 100 0.0595 avg epoch 00:43 total elapsed 43:54 remaining 28:47\n",
      "epoch 61 / 100 0.0491 avg epoch 00:43 total elapsed 44:37 remaining 28:04\n",
      "epoch 62 / 100 0.0480 avg epoch 00:43 total elapsed 45:20 remaining 27:21\n",
      "epoch 63 / 100 0.0411 avg epoch 00:43 total elapsed 46:03 remaining 26:37\n",
      "epoch 64 / 100 0.0295 avg epoch 00:43 total elapsed 46:47 remaining 25:54\n",
      "epoch 65 / 100 0.0199 avg epoch 00:43 total elapsed 47:30 remaining 25:11\n",
      "epoch 66 / 100 0.0235 avg epoch 00:43 total elapsed 48:13 remaining 24:28\n",
      "epoch 67 / 100 0.0189 avg epoch 00:43 total elapsed 48:56 remaining 23:45\n",
      "epoch 68 / 100 0.0175 avg epoch 00:43 total elapsed 49:39 remaining 23:01\n",
      "epoch 69 / 100 0.0252 avg epoch 00:43 total elapsed 50:22 remaining 22:18\n",
      "epoch 70 / 100 0.0230 avg epoch 00:43 total elapsed 51:06 remaining 21:35\n",
      "epoch 71 / 100 0.0151 avg epoch 00:43 total elapsed 51:49 remaining 20:52\n",
      "epoch 72 / 100 0.0176 avg epoch 00:43 total elapsed 52:32 remaining 20:09\n",
      "epoch 73 / 100 0.0095 avg epoch 00:43 total elapsed 53:15 remaining 19:26\n",
      "epoch 74 / 100 0.0099 avg epoch 00:43 total elapsed 53:58 remaining 18:42\n",
      "epoch 75 / 100 0.0285 avg epoch 00:43 total elapsed 54:42 remaining 17:59\n",
      "epoch 76 / 100 0.0381 avg epoch 00:43 total elapsed 55:25 remaining 17:16\n",
      "epoch 77 / 100 0.0486 avg epoch 00:43 total elapsed 56:08 remaining 16:33\n",
      "epoch 78 / 100 0.0364 avg epoch 00:43 total elapsed 56:51 remaining 15:50\n",
      "epoch 79 / 100 0.0261 avg epoch 00:43 total elapsed 57:35 remaining 15:06\n",
      "epoch 80 / 100 0.0243 avg epoch 00:43 total elapsed 58:18 remaining 14:23\n",
      "epoch 81 / 100 0.0263 avg epoch 00:43 total elapsed 59:01 remaining 13:40\n",
      "epoch 82 / 100 0.0303 avg epoch 00:43 total elapsed 59:44 remaining 12:57\n",
      "epoch 83 / 100 0.0138 avg epoch 00:43 total elapsed 00:27 remaining 12:14\n",
      "epoch 84 / 100 0.0077 avg epoch 00:43 total elapsed 01:11 remaining 11:31\n",
      "epoch 85 / 100 0.0110 avg epoch 00:43 total elapsed 01:54 remaining 10:47\n",
      "epoch 86 / 100 0.0165 avg epoch 00:43 total elapsed 02:37 remaining 10:04\n",
      "epoch 87 / 100 0.0148 avg epoch 00:43 total elapsed 03:20 remaining 09:21\n",
      "epoch 88 / 100 0.0287 avg epoch 00:43 total elapsed 04:03 remaining 08:38\n",
      "epoch 89 / 100 0.0169 avg epoch 00:43 total elapsed 04:46 remaining 07:55\n",
      "epoch 90 / 100 0.0192 avg epoch 00:43 total elapsed 05:29 remaining 07:11\n",
      "epoch 91 / 100 0.0219 avg epoch 00:43 total elapsed 06:12 remaining 06:28\n",
      "epoch 92 / 100 0.0120 avg epoch 00:43 total elapsed 06:55 remaining 05:45\n",
      "epoch 93 / 100 0.0177 avg epoch 00:43 total elapsed 07:39 remaining 05:02\n",
      "epoch 94 / 100 0.0196 avg epoch 00:43 total elapsed 08:22 remaining 04:19\n",
      "epoch 95 / 100 0.0229 avg epoch 00:43 total elapsed 09:05 remaining 03:35\n",
      "epoch 96 / 100 0.0173 avg epoch 00:43 total elapsed 09:48 remaining 02:52\n",
      "epoch 97 / 100 0.0101 avg epoch 00:43 total elapsed 10:31 remaining 02:09\n",
      "epoch 98 / 100 0.0123 avg epoch 00:43 total elapsed 11:14 remaining 01:26\n",
      "epoch 99 / 100 0.0113 avg epoch 00:43 total elapsed 11:58 remaining 00:43\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_epochs = 100\n",
    "lossfct = nn.CrossEntropyLoss().to(DEVICE)\n",
    "start_time = time()\n",
    "for i_epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    lstm.train()\n",
    "    classifier.train()\n",
    "    emb.to(DEVICE)\n",
    "    lstm.to(DEVICE)\n",
    "    classifier.to(DEVICE)\n",
    "    for i_step, batch in enumerate(dataloader, 1):\n",
    "        X_batch, lengths, y_batch = (b.to(DEVICE) for b in batch)\n",
    "        # run the word indices through the embedding layer and then the LSTM\n",
    "        embed = emb(X_batch)\n",
    "        \n",
    "        # run the embeddings through the LSTM\n",
    "        packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "        output, *_ = lstm(packed)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # run the encoded last hidden state through the classifier\n",
    "        last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "        output = classifier(last_hidden_states)\n",
    "\n",
    "        loss = lossfct(output, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    delta = time() - start_time\n",
    "    avg_delta = delta / (i_epoch + 1)\n",
    "    remaining = avg_delta * (num_epochs - i_epoch)\n",
    "    print('epoch', i_epoch, '/', num_epochs, f'{train_loss / i_step:.4f}',\n",
    "          f'avg epoch {datetime.fromtimestamp(avg_delta):%M:%S}',\n",
    "          f'total elapsed {datetime.fromtimestamp(delta):%M:%S}',\n",
    "          f'remaining {datetime.fromtimestamp(remaining):%M:%S}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "# turn all the data into integer indices\n",
    "X_test = [[word2idx.get(w, 0) for w in doc.split()] for doc in gnad_test.text]\n",
    "y_test = label_encoder.transform(gnad_test.category)\n",
    "\n",
    "doc_lengths = [len(doc) for doc in X_test]\n",
    "longest_doc = max(doc_lengths)\n",
    "\n",
    "data = np.zeros((len(X_test), longest_doc), dtype=np.int)\n",
    "for i_doc, doc in enumerate(X_test):\n",
    "    data[i_doc, :len(doc)] += doc\n",
    "\n",
    "test_dataset = TensorDataset(torch.LongTensor(data), torch.LongTensor(doc_lengths), torch.LongTensor(y_test))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm.to(DEVICE)\n",
    "pred = []\n",
    "for i_step, (X_batch, lengths, y_batch) in enumerate(test_dataloader, 1):\n",
    "    X_batch = X_batch.to(DEVICE)\n",
    "    # run the word indices through the embedding layer and then the LSTM\n",
    "    embed = emb(X_batch)\n",
    "\n",
    "    # run the embeddings through the LSTM\n",
    "    packed = pack_padded_sequence(embed, lengths, enforce_sorted=False, batch_first=True)\n",
    "    output, *_ = lstm(packed)\n",
    "    output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    # run the encoded last hidden state through the classifier\n",
    "    last_hidden_states = output[torch.arange(0, X_batch.size()[0]), lengths-1, :]\n",
    "    output = classifier(last_hidden_states)\n",
    "\n",
    "    _, pred_ = F.log_softmax(output, dim=-1).max(dim=-1)\n",
    "    pred.extend(pred_.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Etat       0.87      0.69      0.77        67\n",
      "       Inland       0.75      0.75      0.75       102\n",
      "International       0.83      0.85      0.84       151\n",
      "       Kultur       0.81      0.78      0.79        54\n",
      "     Panorama       0.73      0.82      0.78       168\n",
      "        Sport       0.99      0.98      0.99       120\n",
      "          Web       0.92      0.93      0.92       168\n",
      "   Wirtschaft       0.84      0.79      0.82       141\n",
      " Wissenschaft       0.91      0.86      0.88        57\n",
      "\n",
      "     accuracy                           0.84      1028\n",
      "    macro avg       0.85      0.83      0.84      1028\n",
      " weighted avg       0.85      0.84      0.84      1028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, pred, target_names=list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (64, 1-layer) + Linear\n",
    "    100 epochs\n",
    "    SGD lr=0.1 / momentum = 0.9\n",
    "    ~ 25 minutes\n",
    "    ** -----------------------------------------------------\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.80      0.70      0.75        67\n",
    "           Inland       0.66      0.72      0.69       102\n",
    "    International       0.87      0.81      0.84       151\n",
    "           Kultur       0.73      0.74      0.73        54\n",
    "         Panorama       0.78      0.71      0.75       168\n",
    "            Sport       0.95      0.98      0.97       120\n",
    "              Web       0.90      0.89      0.89       168\n",
    "       Wirtschaft       0.81      0.70      0.75       141\n",
    "     Wissenschaft       0.53      0.89      0.66        57\n",
    "\n",
    "         accuracy                           0.80      1028\n",
    "        macro avg       0.78      0.79      0.78      1028\n",
    "     weighted avg       0.81      0.80      0.80      1028\n",
    "     \n",
    "     \n",
    "     \n",
    "\n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (64, 1-layer) + Linear\n",
    "    100 epochs\n",
    "    Adam lr=0.01\n",
    "    ~ 25 minutes\n",
    "    ** -----------------------------------------------------\n",
    "\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.81      0.75      0.78        67\n",
    "           Inland       0.71      0.74      0.72       102\n",
    "    International       0.86      0.83      0.84       151\n",
    "           Kultur       0.85      0.81      0.83        54\n",
    "         Panorama       0.74      0.75      0.74       168\n",
    "            Sport       0.97      0.96      0.97       120\n",
    "              Web       0.89      0.89      0.89       168\n",
    "       Wirtschaft       0.77      0.79      0.78       141\n",
    "     Wissenschaft       0.81      0.88      0.84        57\n",
    "\n",
    "         accuracy                           0.82      1028\n",
    "        macro avg       0.82      0.82      0.82      1028\n",
    "     weighted avg       0.82      0.82      0.82      1028\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (128, 2-layer, bidir) + Linear\n",
    "    fine tuned embeddings\n",
    "    100 epochs\n",
    "    Adam lr=0.01\n",
    "    ~ 55 minutes\n",
    "                  -----------------------------------------------------\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.86      0.76      0.81        67\n",
    "           Inland       0.73      0.78      0.76       102\n",
    "    International       0.88      0.86      0.87       151\n",
    "           Kultur       0.85      0.83      0.84        54\n",
    "         Panorama       0.79      0.76      0.77       168\n",
    "            Sport       0.98      0.98      0.98       120\n",
    "              Web       0.90      0.91      0.91       168\n",
    "       Wirtschaft       0.81      0.80      0.81       141\n",
    "     Wissenschaft       0.82      0.96      0.89        57\n",
    "\n",
    "         accuracy                           0.85      1028\n",
    "        macro avg       0.85      0.85      0.85      1028\n",
    "     weighted avg       0.85      0.85      0.85      1028\n",
    "     \n",
    "    **  ------------------------------------------------------------------------- **\n",
    "    embedding + LSTM (64, 4-layer, bidir) + Linear\n",
    "    fine tuned embeddings\n",
    "    100 epochs\n",
    "    Adam lr=0.01\n",
    "    ~ 70 minutes\n",
    "                  -----------------------------------------------------\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             Etat       0.87      0.69      0.77        67\n",
    "           Inland       0.75      0.75      0.75       102\n",
    "    International       0.83      0.85      0.84       151\n",
    "           Kultur       0.81      0.78      0.79        54\n",
    "         Panorama       0.73      0.82      0.78       168\n",
    "            Sport       0.99      0.98      0.99       120\n",
    "              Web       0.92      0.93      0.92       168\n",
    "       Wirtschaft       0.84      0.79      0.82       141\n",
    "     Wissenschaft       0.91      0.86      0.88        57\n",
    "\n",
    "         accuracy                           0.84      1028\n",
    "        macro avg       0.85      0.83      0.84      1028\n",
    "     weighted avg       0.85      0.84      0.84      1028\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings, Encoder / Decoder Architecture and Transfer Learning\n",
    "\n",
    "- Akbik et al.: [Pooled Contextualized Embeddings for Named Entity Recognition.](https://www.aclweb.org/anthology/N19-1078/) NAACL-HLT (1) 2019: 724-728\n",
    "\n",
    "- Howard et al.: [_Universal Language Model Fine-tuning for Text Classification._](https://www.aclweb.org/anthology/P18-1031/) ACL (1) 2018: 328-339\n",
    "- Jeremy Howard and Sebastian Ruder [Introducing state of the art text classification with universal language models](http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) 15 May 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ULMfit](./img/ulmfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained AWD-LSTM from fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fastai' has no attribute 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-9ba3d10c02a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMDB_SAMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'fastai' has no attribute 'datasets'"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "path = fastai.datasets.untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = text.data.TextLMDataBunch.from_csv(path, 'texts.csv')\n",
    "awd_lstm = text.language_model_learner(data_lm, text.AWD_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck? What made this the xxunk successful triumph it was ? Was it casting , music , imagination , ingenuity , or luck ? No , no , no , no , no , no , no , no ! No , no , no , no'"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm.beam_search('What made this the hugely successful triumph it was? Was it casting, music, imagination, ingenuity, or luck?',\n",
    "                     n_words=25,\n",
    "                     temperature=0.95,\n",
    "                     top_k=25,\n",
    "                     beam_sz=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
